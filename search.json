[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hey there! I‚Äôm Sean Lewis, and I‚Äôm excited to welcome you to my blog.\nFor as long as I can remember, I‚Äôve been passionate about numbers‚Äînot just their abstract beauty but their ability to tell a story, predict the future, and drive smarter decision-making. My journey has taken me from math classrooms to boardrooms, from teaching algebra to deploying machine learning models, and now, to this blog‚Äîwhere I‚Äôll be sharing my experiences, insights, and discoveries in data science, analytics, actuarial modeling, and education.\n\n\n\nI‚Äôm a Consultant Analytic Scientist in Data Analytics, but my career path has been anything but conventional. I started in education, spending five years as a high school math teacher, where I developed a deep appreciation for breaking down complex concepts into clear, digestible insights. Teaching wasn‚Äôt just about equations and formulas‚Äîit was about:\n\nHelping students think critically\nAnalyzing data\nMaking informed decisions\n\nThese are the same skills that drive success in data science today.\n\n\n‚úÖ Math Department Chair & Algebra Team Lead ‚Äì Leading curriculum development and supporting student success.\n‚úÖ SOL Review Committee Representative ‚Äì Ensuring that standardized testing aligned with effective teaching strategies.\n‚úÖ Math SAT Tutor ‚Äì Coaching students to master problem-solving strategies for college admissions.\n‚úÖ Varsity Tennis Coach ‚Äì Because sometimes, data isn‚Äôt just about numbers‚Äîit‚Äôs about strategy, adaptability, and teamwork.\nThese experiences shaped the way I approach problem-solving today. Teaching taught me how to communicate complex ideas, a skill that translates directly into my work as a data scientist‚Äîwhere building models is only half the battle. The real challenge? Translating insights into action.\n\n\n\n\n\nAfter my time in education, I transitioned into actuarial science and data analytics, working on risk modeling, pricing strategy, and predictive analytics in the insurance industry. My career has taken me deep into:\nüìä Actuarial Science & Pricing Models ‚Äì Using statistical techniques to develop risk models, optimize pricing strategies, and improve profitability for various insurance lines.\nü§ñ Machine Learning & Model Deployment ‚Äì Designing and deploying Gradient Boosting Machines (GBMs), Generalized Linear Models (GLMs), and AI-powered predictive models that enhance decision-making.\nüöÄ MLOps & Data Engineering ‚Äì Automating workflows with Databricks, MLFlow, and GitHub CI/CD, ensuring that machine learning models are not just built‚Äîbut operationalized and scaled.\nüìà Business Intelligence & Visualization ‚Äì Transforming raw data into compelling stories with Tableau, Power BI, and Snowflake, helping decision-makers see the bigger picture.\nüîç Bridging Data & Business Strategy ‚Äì I don‚Äôt just build models‚ÄîI work closely with business leaders to turn insights into action, ensuring that analytics drive real-world impact.\nWhether it‚Äôs developing technical price models, improving underwriting profitability, or integrating third-party data to enhance predictions, my work revolves around turning data into decisions.\n\n\n\n\nThis blog is where I break down the complex world of data science, analytics, actuarial modeling, and education into engaging, practical discussions. Expect deep dives into:\n‚úÖ Machine Learning & Model Deployment ‚Äì Best practices for training, validating, and deploying robust models.\n‚úÖ MLOps & CI/CD ‚Äì How to build repeatable, scalable machine learning pipelines.\n‚úÖ Actuarial Science & Pricing ‚Äì A modern take on risk modeling and insurance analytics.\n‚úÖ Data Engineering & Visualization ‚Äì Techniques for handling, processing, and interpreting large datasets.\n‚úÖ Teaching & Communication in Data Science ‚Äì How to explain complex ideas in ways that make sense to everyone.\n\n\n\n\nWe live in a world where data is everywhere, but making sense of it is a skill. Whether you‚Äôre a data scientist, analyst, business leader, or someone just starting out, this blog is for you. My goal is to bridge the gap between technical expertise and real-world application, making analytics accessible, impactful, and‚Äîmost importantly‚Äîactionable.\nIf you love digging into data, building predictive models, or simply understanding the world through numbers, then you‚Äôre in the right place.\nLet‚Äôs explore, learn, and push the boundaries of what‚Äôs possible with data. Welcome aboard! üöÄ"
  },
  {
    "objectID": "posts/welcome/index.html#who-am-i",
    "href": "posts/welcome/index.html#who-am-i",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I‚Äôm a Consultant Analytic Scientist in Data Analytics, but my career path has been anything but conventional. I started in education, spending five years as a high school math teacher, where I developed a deep appreciation for breaking down complex concepts into clear, digestible insights. Teaching wasn‚Äôt just about equations and formulas‚Äîit was about:\n\nHelping students think critically\nAnalyzing data\nMaking informed decisions\n\nThese are the same skills that drive success in data science today.\n\n\n‚úÖ Math Department Chair & Algebra Team Lead ‚Äì Leading curriculum development and supporting student success.\n‚úÖ SOL Review Committee Representative ‚Äì Ensuring that standardized testing aligned with effective teaching strategies.\n‚úÖ Math SAT Tutor ‚Äì Coaching students to master problem-solving strategies for college admissions.\n‚úÖ Varsity Tennis Coach ‚Äì Because sometimes, data isn‚Äôt just about numbers‚Äîit‚Äôs about strategy, adaptability, and teamwork.\nThese experiences shaped the way I approach problem-solving today. Teaching taught me how to communicate complex ideas, a skill that translates directly into my work as a data scientist‚Äîwhere building models is only half the battle. The real challenge? Translating insights into action."
  },
  {
    "objectID": "posts/welcome/index.html#from-the-classroom-to-the-world-of-data",
    "href": "posts/welcome/index.html#from-the-classroom-to-the-world-of-data",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "After my time in education, I transitioned into actuarial science and data analytics, working on risk modeling, pricing strategy, and predictive analytics in the insurance industry. My career has taken me deep into:\nüìä Actuarial Science & Pricing Models ‚Äì Using statistical techniques to develop risk models, optimize pricing strategies, and improve profitability for various insurance lines.\nü§ñ Machine Learning & Model Deployment ‚Äì Designing and deploying Gradient Boosting Machines (GBMs), Generalized Linear Models (GLMs), and AI-powered predictive models that enhance decision-making.\nüöÄ MLOps & Data Engineering ‚Äì Automating workflows with Databricks, MLFlow, and GitHub CI/CD, ensuring that machine learning models are not just built‚Äîbut operationalized and scaled.\nüìà Business Intelligence & Visualization ‚Äì Transforming raw data into compelling stories with Tableau, Power BI, and Snowflake, helping decision-makers see the bigger picture.\nüîç Bridging Data & Business Strategy ‚Äì I don‚Äôt just build models‚ÄîI work closely with business leaders to turn insights into action, ensuring that analytics drive real-world impact.\nWhether it‚Äôs developing technical price models, improving underwriting profitability, or integrating third-party data to enhance predictions, my work revolves around turning data into decisions."
  },
  {
    "objectID": "posts/welcome/index.html#what-youll-find-here",
    "href": "posts/welcome/index.html#what-youll-find-here",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This blog is where I break down the complex world of data science, analytics, actuarial modeling, and education into engaging, practical discussions. Expect deep dives into:\n‚úÖ Machine Learning & Model Deployment ‚Äì Best practices for training, validating, and deploying robust models.\n‚úÖ MLOps & CI/CD ‚Äì How to build repeatable, scalable machine learning pipelines.\n‚úÖ Actuarial Science & Pricing ‚Äì A modern take on risk modeling and insurance analytics.\n‚úÖ Data Engineering & Visualization ‚Äì Techniques for handling, processing, and interpreting large datasets.\n‚úÖ Teaching & Communication in Data Science ‚Äì How to explain complex ideas in ways that make sense to everyone."
  },
  {
    "objectID": "posts/welcome/index.html#why-this-matters",
    "href": "posts/welcome/index.html#why-this-matters",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "We live in a world where data is everywhere, but making sense of it is a skill. Whether you‚Äôre a data scientist, analyst, business leader, or someone just starting out, this blog is for you. My goal is to bridge the gap between technical expertise and real-world application, making analytics accessible, impactful, and‚Äîmost importantly‚Äîactionable.\nIf you love digging into data, building predictive models, or simply understanding the world through numbers, then you‚Äôre in the right place.\nLet‚Äôs explore, learn, and push the boundaries of what‚Äôs possible with data. Welcome aboard! üöÄ"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html",
    "title": "06-Why you should use a Framework (follow Along)",
    "section": "",
    "text": "If you‚Äôve finished going through my Linear model and neural net from scratch notebook, then now is a good time to look at how to do the same thing using a library, instead of doing it from scratch. We‚Äôll use fastai and PyTorch. The benefits of using these libraries is:- Best practices are handled for you automatically ‚Äì fast.ai has done thousands of hours of experiments to figure out what the best settings are for you- Less time getting set up, which means more time to try out your new ideas- Each idea you try will be less work, because fastai and PyTorch will do the many of the menial bits for you- You can always drop down from fastai to PyTorch if you need to customise any part (or drop down from the fastai Application API to the fastai mid or low tier APIs), or even drop down from PyTorch to plain python for deep customisation.Let‚Äôs see how that looks in practice. We‚Äôll start by doing the same library setup as in the ‚Äúfrom scratch‚Äù notebook:\n::: {#cell-3 .cell _kg_hide-output=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.status.busy‚Äù:‚Äú2022-05-16T21:26:52.397584Z‚Äù,‚Äúiopub.execute_input‚Äù:‚Äú2022-05-16T21:26:52.398131Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2022-05-16T21:27:20.193935Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2022-05-16T21:26:52.398019Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2022-05-16T21:27:20.193065Z‚Äù}‚Äô trusted=‚Äòtrue‚Äô}\nfrom pathlib import Pathimport osiskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')if iskaggle:    path = Path('../input/titanic')    !pip install -Uqq fastaielse:    import zipfile,kaggle    path = Path('titanic')    kaggle.api.competition_download_cli(str(path))    zipfile.ZipFile(f'{path}.zip').extractall(path)\n:::\nWe‚Äôll import the fastai tabular library, set a random seed so the notebook is reproducible, and pick a reasonable number of significant figures to display in our tables:\n\nfrom fastai.tabular.all import *pd.options.display.float_format = '{:.2f}'.formatset_seed(42)"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#introduction-and-set-up",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#introduction-and-set-up",
    "title": "06-Why you should use a Framework (follow Along)",
    "section": "",
    "text": "If you‚Äôve finished going through my Linear model and neural net from scratch notebook, then now is a good time to look at how to do the same thing using a library, instead of doing it from scratch. We‚Äôll use fastai and PyTorch. The benefits of using these libraries is:- Best practices are handled for you automatically ‚Äì fast.ai has done thousands of hours of experiments to figure out what the best settings are for you- Less time getting set up, which means more time to try out your new ideas- Each idea you try will be less work, because fastai and PyTorch will do the many of the menial bits for you- You can always drop down from fastai to PyTorch if you need to customise any part (or drop down from the fastai Application API to the fastai mid or low tier APIs), or even drop down from PyTorch to plain python for deep customisation.Let‚Äôs see how that looks in practice. We‚Äôll start by doing the same library setup as in the ‚Äúfrom scratch‚Äù notebook:\n::: {#cell-3 .cell _kg_hide-output=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.status.busy‚Äù:‚Äú2022-05-16T21:26:52.397584Z‚Äù,‚Äúiopub.execute_input‚Äù:‚Äú2022-05-16T21:26:52.398131Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2022-05-16T21:27:20.193935Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2022-05-16T21:26:52.398019Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2022-05-16T21:27:20.193065Z‚Äù}‚Äô trusted=‚Äòtrue‚Äô}\nfrom pathlib import Pathimport osiskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')if iskaggle:    path = Path('../input/titanic')    !pip install -Uqq fastaielse:    import zipfile,kaggle    path = Path('titanic')    kaggle.api.competition_download_cli(str(path))    zipfile.ZipFile(f'{path}.zip').extractall(path)\n:::\nWe‚Äôll import the fastai tabular library, set a random seed so the notebook is reproducible, and pick a reasonable number of significant figures to display in our tables:\n\nfrom fastai.tabular.all import *pd.options.display.float_format = '{:.2f}'.formatset_seed(42)"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#prep-the-data",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#prep-the-data",
    "title": "06-Why you should use a Framework (follow Along)",
    "section": "Prep the data",
    "text": "Prep the data\nWe‚Äôll read the CSV file just like we did before:\n\ndf = pd.read_csv(path/'train.csv')\n\nWhen you do everything from scratch, every bit of feature engineering requires a whole lot of work, since you have to think about things like dummy variables, normalization, missing values, and so on. But with fastai that‚Äôs all done for you. So let‚Äôs go wild and create lots of new features! We‚Äôll use a bunch of the most interesting ones from this fantastic Titanic feature engineering notebook (and be sure to click that link and upvote that notebook if you like it to thank the author for their hard work!)\n\ndef add_features(df):    df['LogFare'] = np.log1p(df['Fare'])    df['Deck'] = df.Cabin.str[0].map(dict(A=\"ABC\", B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\"))    df['Family'] = df.SibSp+df.Parch    df['Alone'] = df.Family==1    df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')    df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]    df['Title'] = df.Title.map(dict(Mr=\"Mr\",Miss=\"Miss\",Mrs=\"Mrs\",Master=\"Master\")).value_counts(dropna=False)add_features(df)\n\nAs we discussed in the last notebook, we can use RandomSplitter to separate out the training and validation sets:\n\nsplits = RandomSplitter(seed=42)(df)\n\nNow the entire process of getting the data ready for training requires just this one cell!:\n\ndls = TabularPandas(    df, splits=splits,    procs = [Categorify, FillMissing, Normalize],    cat_names=[\"Sex\",\"Pclass\",\"Embarked\",\"Deck\", \"Title\"],    cont_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Alone', 'TicketFreq', 'Family'],    y_names=\"Survived\", y_block = CategoryBlock(),).dataloaders(path=\".\")\n\nHere‚Äôs what each of the parameters means:- Use splits for indices of training and validation sets: splits=splits, - Turn strings into categories, fill missing values in numeric columns with the median, normalise all numeric columns: procs = [Categorify, FillMissing, Normalize], - These are the categorical independent variables: cat_names=[‚ÄúSex‚Äù,‚ÄúPclass‚Äù,‚ÄúEmbarked‚Äù,‚ÄúDeck‚Äù, ‚ÄúTitle‚Äù], - These are the continuous independent variables: cont_names=[‚ÄòAge‚Äô, ‚ÄòSibSp‚Äô, ‚ÄòParch‚Äô, ‚ÄòLogFare‚Äô, ‚ÄòAlone‚Äô, ‚ÄòTicketFreq‚Äô, ‚ÄòFamily‚Äô], - This is the dependent variable: y_names=‚ÄúSurvived‚Äù,- The dependent variable is categorical (so build a classification model, not a regression model): y_block = CategoryBlock(),"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#train-the-model",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#train-the-model",
    "title": "06-Why you should use a Framework (follow Along)",
    "section": "Train the model",
    "text": "Train the model\nThe data and model together make up a Learner. To create one, we say what the data is (dls), and the size of each hidden layer ([10,10]), along with any metrics we want to print along the way:\n\nlearn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n\nYou‚Äôll notice we didn‚Äôt have to do any messing around to try to find a set of random coefficients that will train correctly ‚Äì that‚Äôs all handled automatically.One handy feature that fastai can also tell us what learning rate to use:\n\nlearn.lr_find(suggest_funcs=(slide, valley))\n\nThe two colored points are both reasonable choices for a learning rate. I‚Äôll pick somewhere between the two (0.03) and train for a few epochs:\n\nlearn.fit(16, lr=0.03)\n\nWe‚Äôve got a similar accuracy to our previous ‚Äúfrom scratch‚Äù model ‚Äì which isn‚Äôt too surprising, since as we discussed, this dataset is too small and simple to really see much difference. A simple linear model already does a pretty good job. But that‚Äôs OK ‚Äì the goal here is to show you how to get started with deep learning and understand how it really works, and the best way to do that is on small and easy to understand datasets."
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#submit-to-kaggle",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#submit-to-kaggle",
    "title": "06-Why you should use a Framework (follow Along)",
    "section": "Submit to Kaggle",
    "text": "Submit to Kaggle\nOne important feature of fastai is that all the information needed to apply the data transformations and the model to a new dataset are stored in the learner. You can call export to save it to a file to use it later in production, or you can use the trained model right away to get predictions on a test set.To submit to Kaggle, we‚Äôll need to read in the test set, and do the same feature engineering we did for the training set:\n\ntst_df = pd.read_csv(path/'test.csv')tst_df['Fare'] = tst_df.Fare.fillna(0)add_features(tst_df)\n\nBut we don‚Äôt need to manually specify any of the processing steps necessary to get the data ready for modeling, since that‚Äôs all saved in the learner. To specify we want to apply the same steps to a new dataset, use the test_dl() method:\n\ntst_dl = learn.dls.test_dl(tst_df)\n\nNow we can use get_preds to get the predictions for the test set:\n\npreds,_ = learn.get_preds(dl=tst_dl)\n\nFinally, let‚Äôs create a submission CSV just like we did in the previous notebook‚Ä¶\n\ntst_df['Survived'] = (preds[:,1]&gt;0.5).int()sub_df = tst_df[['PassengerId','Survived']]sub_df.to_csv('sub.csv', index=False)\n\n‚Ä¶and check that it looks reasonable:\n\n!head sub.csv"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#ensembling",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#ensembling",
    "title": "06-Why you should use a Framework (follow Along)",
    "section": "Ensembling",
    "text": "Ensembling\nSince it‚Äôs so easy to create a model now, it‚Äôs easier to play with more advanced modeling approaches. For instance, we can create five separate models, each trained from different random starting points, and average them. This is the simplest approach of ensembling models, which combines multiple models to generate predictions that are better than any of the single models in the ensemble.To create our ensemble, first we copy the three steps we used above to create and train a model, and apply it to the test set:\n\ndef ensemble():    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])    with learn.no_bar(),learn.no_logging(): learn.fit(16, lr=0.03)    return learn.get_preds(dl=tst_dl)[0]\n\nNow we run this five times, and collect the results into a list:\n\nlearns = [ensemble() for _ in range(5)]\n\nWe stack this predictions together and take their average predictions:\n\nens_preds = torch.stack(learns).mean(0)\n\nFinally, use the same code as before to generate a submission file, which we can submit to Kaggle after the notebook is saved and run:\n\ntst_df['Survived'] = (ens_preds[:,1]&gt;0.5).int()sub_df = tst_df[['PassengerId','Survived']]sub_df.to_csv('ens_sub.csv', index=False)\n\nAt the time of writing, this submission is well within the top 25% of entries to the competition.(A lot of submissions to this competition use additional external data, but we have restricted ourselves to just using the data provided. We‚Äôd probably do a lot better if we used external data too. Feel free to give that a try, and see how you go. Note that you‚Äôll never be able to get to the top of the leaderboard, since a lot of folks in this competition have cheated, by downloading the answers from the internet and uploading them as their submission. In a real competition that‚Äôs not possible, because the answers aren‚Äôt public, but there‚Äôs nothing stopping people from cheating in a tutorial/practice competition like this one. So if you‚Äôre ready for a real challenge, take a look at the competitions page and start working on a real competition!)"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#final-thoughts",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#final-thoughts",
    "title": "06-Why you should use a Framework (follow Along)",
    "section": "Final thoughts",
    "text": "Final thoughts\nAs you can see, using fastai and PyTorch made things much easier than doing it from scratch, but it also hid away a lot of the details. So if you only ever use a framework, you‚Äôre not going to as fully understand what‚Äôs going on under the hood. That understanding can be really helpful when it comes to debugging and improving your models. But do use fastai when you‚Äôre creating models on Kaggle or in ‚Äúreal life‚Äù, because otherwise you‚Äôre not taking advantage of all the research that‚Äôs gone into optimising the models for you, and you‚Äôll end up spending more time debugging and implementing menial boiler-plate than actually solving the real problem!If you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you‚Äôre looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won‚Äôt get counted!) And if you have any questions or comments, please pop them below ‚Äì I read every comment I receive!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About This Blog\nWelcome to my blog! This is a space where data science meets strategy, where numbers tell stories, and where analytics drive action.\nHere, I break down machine learning, actuarial modeling, and business intelligence into practical, engaging discussions. Whether it‚Äôs model deployment, MLOps, or pricing analytics, this blog is about turning raw data into insights that matter.\nAs Kevin Durant once said: &gt; ‚ÄúEverything in life has a rhythm, and everything in life has a pace.‚Äù &gt; ‚Äî Kevin Durant, The Boardroom Interview, New York, March 2019\nData science is no different‚Äîwhether optimizing machine learning models or refining pricing strategies, timing, iteration, and adaptation are key. Success isn‚Äôt just about raw processing power; it‚Äôs about finding the right rhythm between exploration and execution.\nStay tuned for deep dives into data science, visualization, and AI-driven decision-making. Let‚Äôs explore the numbers that shape our world! üöÄ"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "seanlewis08.github.io",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n05-Linear Model & NN From Scratch (Follow ALong)\n\n\n\n\n\n\nnueral net\n\n\nfrom scratch\n\n\n\nThis is the first notebook in the fast.ai lecture for #5\n\n\n\n\n\nFeb 23, 2025\n\n\n16 min\n\n\n\n\n\n\n\n06-Why you should use a Framework (follow Along)\n\n\n\n\n\n\nnueral net\n\n\nfrom scratch\n\n\n\nThis is the second notebook in the fast.ai lecture for #5\n\n\n\n\n\nFeb 23, 2025\n\n\n7 min\n\n\n\n\n\n\n\n07-How Random Forests Really Work\n\n\n\n\n\n\nnueral net\n\n\nfrom scratch\n\n\n\nThis is the third notebook in the fast.ai lecture for #5\n\n\n\n\n\nFeb 23, 2025\n\n\n13 min\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nabout me\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "",
    "text": "import os\nfrom pathlib import Path\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path('../input/titanic')\nelse:\n    path = Path('titanic')\n    if not path.exists():\n        import zipfile,kaggle\n        kaggle.api.competition_download_cli(str(path))\n        zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to D:\\Sean\\DataSpellProjects\\seanlewis08.github.io\\posts\\from_scratch_model\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34.1k/34.1k [00:00&lt;00:00, 5.64MB/s]\n\n\n\n\n\n\n\n\nNote that the data for Kaggle comps always lives in the ../input folder. The easiest way to get the path is to click the ‚ÄúK‚Äù button in the top-right of the Kaggle notebook, click on the folder shown there, and click the copy button.\nWe‚Äôll be using numpy and pytorch for array calculations in this notebook, and pandas for working with tabular data, so we‚Äôll import them and set them to display using a bit more space than they default to.\n\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#introduction",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#introduction",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "",
    "text": "import os\nfrom pathlib import Path\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path('../input/titanic')\nelse:\n    path = Path('titanic')\n    if not path.exists():\n        import zipfile,kaggle\n        kaggle.api.competition_download_cli(str(path))\n        zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nDownloading titanic.zip to D:\\Sean\\DataSpellProjects\\seanlewis08.github.io\\posts\\from_scratch_model\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34.1k/34.1k [00:00&lt;00:00, 5.64MB/s]\n\n\n\n\n\n\n\n\nNote that the data for Kaggle comps always lives in the ../input folder. The easiest way to get the path is to click the ‚ÄúK‚Äù button in the top-right of the Kaggle notebook, click on the folder shown there, and click the copy button.\nWe‚Äôll be using numpy and pytorch for array calculations in this notebook, and pandas for working with tabular data, so we‚Äôll import them and set them to display using a bit more space than they default to.\n\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#cleaning-the-data",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#cleaning-the-data",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nThis is a tabular data competition ‚Äì the data is in the form of a table. It‚Äôs provided as a Comma Separated Values (CSV) file. We can open it using the pandas library, which will create a DataFrame.\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows √ó 12 columns\n\n\n\nAs we learned in the How does a neural net really work notebook, we going to want to multiply each column by some coefficients. But we can see in the Cabin column that there are NaN values, which is how Pandas refers to missing values. We can‚Äôt multiply something by a missing value!\nLet‚Äôs check which columns contain NaN values. Pandas‚Äô isna() function returns True (which is treated as 1 when used as a number) for NaN values, so we can just add them up for each column:\n\ndf.isna().sum()\n\nNotice that by default Pandas sums over columns.\nWe‚Äôll need to replace the missing values with something. It doesn‚Äôt generally matter too much what we choose. We‚Äôll use the most common value (the ‚Äúmode‚Äù). We can use the mode function for that. One wrinkle is that it returns more than one row in the case of ties, so we just grab the first row with iloc[0]:\n\nmodes = df.mode().iloc[0]\nmodes\n\nBTW, it‚Äôs never a good idea to use functions without understanding them. So be sure to google for anything you‚Äôre not familiar with. E.g if you want to learn about iloc (which is a very important function indeed!) then Google will give you a link to a great tutorial.\nNow that we‚Äôve got the mode of each column, we can use fillna to replace the missing values with the mode of each column. We‚Äôll do it ‚Äúin place‚Äù ‚Äì meaning that we‚Äôll change the dataframe itself, rather than returning a new one.\n\ndf.fillna(modes, inplace=True)\n\nWe can now check there‚Äôs no missing values left:\n\ndf.isna().sum()\n\nHere‚Äôs how we get a quick summary of all the numeric columns in the dataset:\n\nimport numpy as np\n\ndf.describe(include=(np.number))\n\nWe can see that Fare contains mainly values of around 0 to 30, but there‚Äôs a few really big ones. This is very common with fields contain monetary values, and it can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will dominate the result.\nYou can see the issue most clearly visually by looking at a histogram, which shows a long tail to the right (and don‚Äôt forget: if you‚Äôre not entirely sure what a histogram is, Google ‚Äúhistogram tutorial‚Äù and do a bit of reading before continuing on):\n\ndf['Fare'].hist();\n\nTo fix this, the most common approach is to take the logarithm, which squishes the big numbers and makes the distribution more reasonable. Note, however, that there are zeros in the Fare column, and log(0) is infinite ‚Äì to fix this, we‚Äôll simply add 1 to all values first:\n\ndf['LogFare'] = np.log(df['Fare']+1)\n\nThe histogram now shows a more even distribution of values without the long tail:\n\ndf['LogFare'].hist();\n\nIt looks from the describe() output like Pclass contains just 3 values, which we can confirm by looking at the Data Dictionary (which you should always study carefully for any project!) ‚Äì\n\npclasses = sorted(df.Pclass.unique())\npclasses\n\nHere‚Äôs how we get a quick summary of all the non-numeric columns in the dataset:\n\ndf.describe(include=[object])\n\nClearly we can‚Äôt multiply strings like male or S by coefficients, so we need to replace those with numbers.\nWe do that by creating new columns containing dummy variables. A dummy variable is a column that contains a 1 where a particular column contains a particular value, or a 0 otherwise. For instance, we could create a dummy variable for Sex='male', which would be a new column containing 1 for rows where Sex is 'male', and 0 for rows where it isn‚Äôt.\nPandas can create these automatically using get_dummies, which also remove the original columns. We‚Äôll create dummy variables for Pclass, even although it‚Äôs numeric, since the numbers 1, 2, and 3 correspond to first, second, and third class cabins - not to counts or measures that make sense to multiply by. We‚Äôll also create dummies for Sex and Embarked since we‚Äôll want to use those as predictors in our model. On the other hand, Cabin, Name, and Ticket have too many unique values for it to make sense creating dummy variables for them.\n\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ndf.columns\n\nWe can see that 5 columns have been added to the end ‚Äì one for each of the possible values of each of the three columns we requested, and that those three requested columns have been removed.\nHere‚Äôs what the first few rows of those newly added columns look like:\n\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ndf[added_cols].head()\n\nNow we can create our independent (predictors) and dependent (target) variables. They both need to be PyTorch tensors. Our dependent variable is Survived:\n\nfrom torch import tensor\n\nt_dep = tensor(df.Survived)\n\nOur independent variables are all the continuous variables of interest plus all the dummy variables we just created:\n\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep\n\nHere‚Äôs the number of rows and columns we have for our independent variables:\n\nt_indep.shape"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#setting-up-a-linear-model",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#setting-up-a-linear-model",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nNow that we‚Äôve got a matrix of independent variables and a dependent variable vector, we can work on calculating our predictions and our loss. In this section, we‚Äôre going to manually do a single step of calculating predictions and loss for every row of our data.\nOur first model will be a simple linear model. We‚Äôll need a coefficient for each column in t_indep. We‚Äôll pick random numbers in the range (-0.5,0.5), and set our manual seed so that my explanations in the prose in this notebook will be consistent with what you see when you run it.\n\ntorch.manual_seed(442)\n\nn_coeff = t_indep.shape[1]\ncoeffs = torch.rand(n_coeff)-0.5\ncoeffs\n\nOur predictions will be calculated by multiplying each row by the coefficients, and adding them up. One interesting point here is that we don‚Äôt need a separate constant term (also known as a ‚Äúbias‚Äù or ‚Äúintercept‚Äù term), or a column of all 1s to give the same effect has having a constant term. That‚Äôs because our dummy variables already cover the entire dataset ‚Äì e.g.¬†there‚Äôs a column for ‚Äúmale‚Äù and a column for ‚Äúfemale‚Äù, and everyone in the dataset is in exactly one of these; therefore, we don‚Äôt need a separate intercept term to cover rows that aren‚Äôt otherwise part of a column.\nHere‚Äôs what the multiplication looks like:\n\nt_indep*coeffs\n\nWe can see we‚Äôve got a problem here. The sums of each row will be dominated by the first column, which is Age, since that‚Äôs bigger on average than all the others.\nLet‚Äôs make all the columns contain numbers from 0 to 1, by dividing each column by its max():\n\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\n\nAs we see, that removes the problem of one column dominating all the others:\n\nt_indep*coeffs\n\nOne thing you hopefully noticed is how amazingly cool this line of code is:\nt_indep = t_indep / vals\nThat is dividing a matrix by a vector ‚Äì what on earth does that mean?!? The trick here is that we‚Äôre taking advantage of a technique in numpy and PyTorch (and many other languages, going all the way back to APL) called broadcasting. In short, this acts as if there‚Äôs a separate copy of the vector for every row of the matrix, so it divides each row of the matrix by the vector. In practice, it doesn‚Äôt actually make any copies, and does the whole thing in a highly optimized way, taking full advantage of modern CPUs (or, indeed, GPUs, if we‚Äôre using them). Broadcasting is one of the most important techniques for making your code concise, maintainable, and fast, so it‚Äôs well worth studying and practicing.\nWe can now create predictions from our linear model, by adding up the rows of the product:\n\npreds = (t_indep*coeffs).sum(axis=1)\n\nLet‚Äôs take a look at the first few:\n\npreds[:10]\n\nOf course, these predictions aren‚Äôt going to be any use, since our coefficients are random ‚Äì they‚Äôre just a starting point for our gradient descent process.\nTo do gradient descent, we need a loss function. Taking the average error of the rows (i.e.¬†the absolute value of the difference between the prediction and the dependent) is generally a reasonable approach:\n\nloss = torch.abs(preds-t_dep).mean()\nloss\n\nNow that we‚Äôve tested out a way of calculating predictions, and loss, let‚Äôs pop them into functions to make life easier:\n\ndef calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#doing-a-gradient-descent-step",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#doing-a-gradient-descent-step",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Doing a gradient descent step",
    "text": "Doing a gradient descent step\nIn this section, we‚Äôre going to do a single ‚Äúepoch‚Äù of gradient descent manually. The only thing we‚Äôre going to automate is calculating gradients, because let‚Äôs face it that‚Äôs pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we‚Äôll need to call requires_grad_() on our coeffs (if you‚Äôre not sure why, review the previous notebook, How does a neural net really work?, before continuing):\n\ncoeffs.requires_grad_()\n\nNow when we calculate our loss, PyTorch will keep track of all the steps, so we‚Äôll be able to get the gradients afterwards:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss\n\nUse backward() to ask PyTorch to calculate gradients now:\n\nloss.backward()\n\nLet‚Äôs see what they look like:\n\ncoeffs.grad\n\nNote that each time we call backward, the gradients are actually added to whatever is in the .grad attribute. Let‚Äôs try running the above steps again:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\ncoeffs.grad\n\nAs you see, our .grad values are have doubled. That‚Äôs because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero.\nWe can now do one gradient descent step, and check that our loss decreases:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)\n    coeffs.grad.zero_()\n    print(calc_loss(coeffs, t_indep, t_dep))\n\nNote that a.sub_(b) subtracts b from a in-place. In PyTorch, any method that ends in _ changes its object in-place. Similarly, a.zero_() sets all elements of a tensor to zero."
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#training-the-linear-model",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#training-the-linear-model",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Training the linear model",
    "text": "Training the linear model\nBefore we begin training our model, we‚Äôll need to ensure that we hold out a validation set for calculating our metrics (for details on this, see ‚ÄúGetting started with NLP for absolute beginners‚Äù.\nThere‚Äôs lots of different ways we can do this. In the next notebook we‚Äôll be comparing our approach here to what the fastai library does, so we‚Äôll want to ensure we split the data in the same way. So let‚Äôs use RandomSplitter to get indices that will split our data into training and validation sets:\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\nNow we can apply those indicies to our independent and dependent variables:\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\nWe‚Äôll create functions for the three things we did manually above: updating coeffs, doing one full gradient descent step, and initilising coeffs to random numbers:\n\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad(): update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\n\ndef init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n\nWe can now use these functions to train our model:\n\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nLet‚Äôs try it. Our loss will print at the end of every step, so we hope we‚Äôll see it going down:\n\ncoeffs = train_model(18, lr=0.2)\n\nIt does!\nLet‚Äôs take a look at the coefficients for each column:\n\ndef show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\nshow_coeffs()"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#measuring-accuracy",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#measuring-accuracy",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Measuring accuracy",
    "text": "Measuring accuracy\nThe Kaggle competition is not, however, scored by absolute error (which is our loss function). It‚Äôs scored by accuracy ‚Äì the proportion of rows where we correctly predict survival. Let‚Äôs see how accurate we were on the validation set. First, calculate the predictions:\n\npreds = calc_preds(coeffs, val_indep)\n\nWe‚Äôll assume that any passenger with a score of over 0.5 is predicted to survive. So that means we‚Äôre correct for each row where preds&gt;0.5 is the same as the dependent variable:\n\nresults = val_dep.bool()==(preds&gt;0.5)\nresults[:16]\n\nLet‚Äôs see what our average accuracy is:\n\nresults.float().mean()\n\nThat‚Äôs not a bad start at all! We‚Äôll create a function so we can calcuate the accuracy easy for other models we train:\n\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)&gt;0.5)).float().mean()\nacc(coeffs)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#using-sigmoid",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#using-sigmoid",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Using sigmoid",
    "text": "Using sigmoid\nLooking at our predictions, there‚Äôs one obvious problem ‚Äì some of our predictions of the probability of survival are &gt;1, and some are &lt;0:\n\npreds[:28]\n\nTo fix this, we should pass every prediction through the sigmoid function, which has a minimum at zero and maximum at one, and is defined as follows:\n\nimport sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));\n\nPyTorch already defines that function for us, so we can modify calc_preds to use it:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))\n\nLet‚Äôs train a new model now, using this updated function to calculate predictions:\n\ncoeffs = train_model(lr=100)\n\nThe loss has improved by a lot. Let‚Äôs check the accuracy:\n\nacc(coeffs)\n\nThat‚Äôs improved too! Here‚Äôs the coefficients of our trained model:\n\nshow_coeffs()\n\nThese coefficients seem reasonable ‚Äì in general, older people and males were less likely to survive, and first class passengers were more likely to survive."
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#submitting-to-kaggle",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#submitting-to-kaggle",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Submitting to Kaggle",
    "text": "Submitting to Kaggle\nNow that we‚Äôve got a trained model, we can prepare a submission to Kaggle. To do that, first we need to read the test set:\n\ntst_df = pd.read_csv(path/'test.csv')\n\nIn this case, it turns out that the test set is missing Fare for one passenger. We‚Äôll just fill it with 0 to avoid problems:\n\ntst_df['Fare'] = tst_df.Fare.fillna(0)\n\nNow we can just copy the same steps we did to our training set and do the same exact things on our test set to preprocess the data:\n\ntst_df.fillna(modes, inplace=True)\ntst_df['LogFare'] = np.log(tst_df['Fare']+1)\ntst_df = pd.get_dummies(tst_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n\ntst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)\ntst_indep = tst_indep / vals\n\nLet‚Äôs calculate our predictions of which passengers survived in the test set:\n\ntst_df['Survived'] = (calc_preds(tst_indep, coeffs)&gt;0.5).int()\n\nThe sample submission on the Kaggle competition site shows that we‚Äôre expected to upload a CSV with just PassengerId and Survived, so let‚Äôs create that and save it:\n\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df.to_csv('sub.csv', index=False)\n\nWe can check the first few rows of the file to make sure it looks reasonable:\n\n!head sub.csv\n\nWhen you click ‚Äúsave version‚Äù in Kaggle, and wait for the notebook to run, you‚Äôll see that sub.csv appears in the ‚ÄúData‚Äù tab. Clicking on that file will show a Submit button, which allows you to submit to the competition."
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#using-matrix-product",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#using-matrix-product",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Using matrix product",
    "text": "Using matrix product\nWe can make things quite a bit neater‚Ä¶\nTake a look at the inner-most calculation we‚Äôre doing to get the predictions:\n\n(val_indep*coeffs).sum(axis=1)\n\nMultiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the @ operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so:\n\nval_indep@coeffs\n\nIt also turns out that this is much faster, because matrix products in PyTorch are very highly optimised.\nLet‚Äôs use this to replace how calc_preds works:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)\n\nIn order to do matrix-matrix products (which we‚Äôll need in the next section), we need to turn coeffs into a column vector (i.e.¬†a matrix with a single column), which we can do by passing a second argument 1 to torch.rand(), indicating that we want our coefficients to have one column:\n\ndef init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\n\nWe‚Äôll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position:\n\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\nWe can now train our model as before and confirm we get identical outputs‚Ä¶:\n\ncoeffs = train_model(lr=100)\n\n‚Ä¶and identical accuracy:\n\nacc(coeffs)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#a-neural-network",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#a-neural-network",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "A neural network",
    "text": "A neural network\nWe‚Äôve now got what we need to implement our neural network.\nFirst, we‚Äôll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like ‚Äì a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We‚Äôll divide these coefficients by n_hidden so that when we sum them up in the next layer we‚Äôll end up with similar magnitude numbers to what we started with.\nThen our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added.\n\ndef init_coeffs(n_hidden=20):\n    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n    layer2 = torch.rand(n_hidden, 1)-0.3\n    const = torch.rand(1)[0]\n    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()\n\nNow we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that‚Äôs our non-linearity), and the second is passed to torch.sigmoid as before.\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    l1,l2,const = coeffs\n    res = F.relu(indeps@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res)\n\nFinally, now that we have more than one set of coefficients, we need to add a loop to update each one:\n\ndef update_coeffs(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nThat‚Äôs it ‚Äì we‚Äôre now ready to train our model!\n\ncoeffs = train_model(lr=1.4)\n\n\ncoeffs = train_model(lr=20)\n\nIt‚Äôs looking good ‚Äì our loss is lower than before. Let‚Äôs see if that translates to a better result on the validation set:\n\nacc(coeffs)\n\nIn this case our neural net isn‚Äôt showing better results than the linear model. That‚Äôs not surprising; this dataset is very small and very simple, and isn‚Äôt the kind of thing we‚Äôd expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#deep-learning",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#deep-learning",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Deep learning",
    "text": "Deep learning\nThe neural net in the previous section only uses one hidden layer, so it doesn‚Äôt count as ‚Äúdeep‚Äù learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.\nFirst, we‚Äôll need to create additional coefficients for each layer:\n\ndef init_coeffs():\n    hiddens = [10, 10]  # &lt;-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\nYou‚Äôll notice here that there‚Äôs a lot of messy constants to get the random numbers in just the right ranges. When you train the model in a moment, you‚Äôll see that the tiniest changes to these initialisations can cause our model to fail to train at all! This is a key reason that deep learning failed to make much progress in the early days ‚Äì it‚Äôs very finicky to get a good starting point for our coefficients. Nowadays, we have ways to deal with that, which we‚Äôll learn about in other notebooks.\nOur deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately:\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.sigmoid(res)\n\nWe also need a minor update to update_coeffs since we‚Äôve got layers and consts separated now:\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nLet‚Äôs train our model‚Ä¶\n\ncoeffs = train_model(lr=4)\n\n‚Ä¶and check its accuracy:\n\nacc(coeffs)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#final-thoughts",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#final-thoughts",
    "title": "05-Linear Model & NN From Scratch (Follow ALong)",
    "section": "Final thoughts",
    "text": "Final thoughts\nIt‚Äôs actually pretty cool that we‚Äôve managed to create a real deep learning model from scratch and trained it to get over 80% accuracy on this task, all in the course of a single notebook!\nThe ‚Äúreal‚Äù deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you‚Äôll recognise the basic steps are the same.\nThe biggest differences in practical models to what we have above are:\n\nHow initialisation and normalisation is done to ensure the model trains correctly every time\nRegularization (to avoid over-fitting)\nModifying the neural net itself to take advantage of knowledge of the problem domain\nDoing gradient descent steps on smaller batches, rather than the whole dataset.\n\nI‚Äôll be adding notebooks about all these later, and will add links here once they‚Äôre ready.\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you‚Äôre looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won‚Äôt get counted!) And if you have any questions or comments, please pop them below ‚Äì I read every comment I receive!"
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html",
    "title": "07-How Random Forests Really Work",
    "section": "",
    "text": "Previously I‚Äôve shown how to create a linear model and neural net from scratch, and used it to create a solid submission to Kaggle‚Äôs Titanic competition. However, for tabular data (i.e data that looks like spreadsheet or database tables, such as the data for the Titanic competition) it‚Äôs more common to see good results by using ensembles of decision trees, such as Random Forests and Gradient Boosting Machines.\nIn this notebook, we‚Äôre going to learn all about Random Forests, by building one from scratch, and using it to submit to the Titanic competition! That might sound like a pretty big stretch, but I think you‚Äôll be surprised to discover how straightforward it actually is.\nWe‚Äôll start by importing the basic set of libraries we normally need for data science work, and setting numpy to use our display space more efficiently:\n::: {#cell-3 .cell _cell_guid=‚Äòb1076dfc-b9ad-4769-8c92-a6c4dae69d19‚Äô _uuid=‚Äò8f2839f25d086af736a60e9eeb907d3b93b6e0e5‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2022-05-23T04:37:24.640765Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2022-05-23T04:37:24.640339Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2022-05-23T04:37:25.174055Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2022-05-23T04:37:25.172992Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2022-05-23T04:37:24.640663Z‚Äù}‚Äô execution_count=1}\nfrom fastai.imports import *\nnp.set_printoptions(linewidth=130)\n:::"
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#introduction",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#introduction",
    "title": "07-How Random Forests Really Work",
    "section": "",
    "text": "Previously I‚Äôve shown how to create a linear model and neural net from scratch, and used it to create a solid submission to Kaggle‚Äôs Titanic competition. However, for tabular data (i.e data that looks like spreadsheet or database tables, such as the data for the Titanic competition) it‚Äôs more common to see good results by using ensembles of decision trees, such as Random Forests and Gradient Boosting Machines.\nIn this notebook, we‚Äôre going to learn all about Random Forests, by building one from scratch, and using it to submit to the Titanic competition! That might sound like a pretty big stretch, but I think you‚Äôll be surprised to discover how straightforward it actually is.\nWe‚Äôll start by importing the basic set of libraries we normally need for data science work, and setting numpy to use our display space more efficiently:\n::: {#cell-3 .cell _cell_guid=‚Äòb1076dfc-b9ad-4769-8c92-a6c4dae69d19‚Äô _uuid=‚Äò8f2839f25d086af736a60e9eeb907d3b93b6e0e5‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2022-05-23T04:37:24.640765Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2022-05-23T04:37:24.640339Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2022-05-23T04:37:25.174055Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2022-05-23T04:37:25.172992Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2022-05-23T04:37:24.640663Z‚Äù}‚Äô execution_count=1}\nfrom fastai.imports import *\nnp.set_printoptions(linewidth=130)\n:::"
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#data-preprocessing",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#data-preprocessing",
    "title": "07-How Random Forests Really Work",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nWe‚Äôll create DataFrames from the CSV files just like we did in the ‚Äúlinear model and neural net from scratch‚Äù notebook, and do much the same preprocessing (so go back and check that out if you‚Äôre not already familiar with the dataset):\n\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle: path = Path('../input/titanic')\nelse:\n    import zipfile,kaggle\n    path = Path('titanic')\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\nmodes = df.mode().iloc[0]\n\ntitanic.zip: Skipping, found more recently modified local copy (use --force to force download)\n\n\nOne difference with Random Forests however is that we don‚Äôt generally have to create dummy variables like we did for non-numeric columns in the linear models and neural network. Instead, we can just convert those fields to categorical variables, which internally in Pandas makes a list of all the unique values in the column, and replaces each value with a number. The number is just an index for looking up the value in the list of all unique values.\n\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True)\n    df['LogFare'] = np.log1p(df['Fare'])\n    df['Embarked'] = pd.Categorical(df.Embarked)\n    df['Sex'] = pd.Categorical(df.Sex)\n\nproc_data(df)\nproc_data(tst_df)\n\nWe‚Äôll make a list of the continuous, categorical, and dependent variables. Note that we no longer consider Pclass a categorical variable. That‚Äôs because it‚Äôs ordered (i.e 1st, 2nd, and 3rd class have an order), and decision trees, as we‚Äôll see, only care about order, not about absolute value.\n\ncats=[\"Sex\",\"Embarked\"]\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\ndep=\"Survived\"\n\nEven although we‚Äôve made the cats columns categorical, they are still shown by Pandas as their original values:\n\ndf.Sex.head()\n\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']\n\n\nHowever behind the scenes they‚Äôre now stored as integers, with indices that are looked up in the Categories list shown in the output above. We can view the stored values by looking in the cat.codes attribute:\n\ndf.Sex.cat.codes.head()\n\n0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8"
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#binary-splits",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#binary-splits",
    "title": "07-How Random Forests Really Work",
    "section": "Binary splits",
    "text": "Binary splits\nBefore we create a Random Forest or Gradient Boosting Machine, we‚Äôll first need to learn how to create a decision tree, from which both of these models are built.\nAnd to create a decision tree, we‚Äôll first need to create a binary split, since that‚Äôs what a decision tree is built from.\nA binary split is where all rows are placed into one of two groups, based on whether they‚Äôre above or below some threshold of some column. For example, we could split the rows of our dataset into males and females, by using the threshold 0.5 and the column Sex (since the values in the column are 0 for female and 1 for male). We can use a plot to see how that would split up our data ‚Äì we‚Äôll use the Seaborn library, which is a layer on top of matplotlib that makes some useful charts easier to create, and more aesthetically pleasing by default:\n\nimport seaborn as sns\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");\n\n\n\n\n\n\n\n\nHere we see that (on the left) if we split the data into males and females, we‚Äôd have groups that have very different survival rates: &gt;70% for females, and &lt;20% for males. We can also see (on the right) that the split would be reasonably even, with over 300 passengers (out of around 900) in each group.\nWe could create a very simple ‚Äúmodel‚Äù which simply says that all females survive, and no males do. To do so, we better first split our data into a training and validation set, to see how accurate this approach turns out to be:\n\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df, test_size=0.25)\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n\n(In the previous step we also replaced the categorical variables with their integer codes, since some of the models we‚Äôll be building in a moment require that.)\nNow we can create our independent variables (the x variables) and dependent (the y variable):\n\ndef xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n\nHere‚Äôs the predictions for our extremely simple model, where female is coded as 0:\n\npreds = val_xs.Sex==0\n\nWe‚Äôll use mean absolute error to measure how good this model is:\n\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)\n\n0.21524663677130046\n\n\nAlternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work ‚Äì here‚Äôs an example of how we could look at LogFare:\n\ndf_fare = trn_df[trn_df.LogFare&gt;0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n\n\n\n\n\n\n\n\nThe boxenplot above shows quantiles of LogFare for each group of Survived==0 and Survived==1. It shows that the average LogFare for passengers that didn‚Äôt survive is around 2.5, and for those that did it‚Äôs around 3.2. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\nLet‚Äôs create a simple model based on this observation:\n\npreds = val_xs.LogFare&gt;2.7\n\n‚Ä¶and test it out:\n\nmean_absolute_error(val_y, preds)\n\n0.336322869955157\n\n\nThis is quite a bit less accurate than our model that used Sex as the single binary split.\nIdeally, we‚Äôd like some way to try more columns and breakpoints more easily. We could create a function that returns how good our model is, in order to more quickly try out a few different splits. We‚Äôll create a score function to do this. Instead of returning the mean absolute error, we‚Äôll calculate a measure of impurity ‚Äì that is, how much the binary split creates two groups where the rows in a group are each similar to each other, or dissimilar.\nWe can measure the similarity of rows inside a group by taking the standard deviation of the dependent variable. If it‚Äôs higher, then it means the rows are more different to each other. We‚Äôll then multiply this by the number of rows, since a bigger group has more impact than a smaller group:\n\ndef _side_score(side, y):\n    tot = side.sum()\n    if tot&lt;=1: return 0\n    return y[side].std()*tot\n\nNow we‚Äôve got that written, we can calculate the score for a split by adding up the scores for the ‚Äúleft hand side‚Äù (lhs) and ‚Äúright hand side‚Äù (rhs):\n\n    \ndef score(col, y, split):\n    lhs = col&lt;=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)\n\nFor instance, here‚Äôs the impurity score for the split on Sex:\n\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n\n0.40787530982063946\n\n\n‚Ä¶and for LogFare:\n\nscore(trn_xs[\"LogFare\"], trn_y, 2.7)\n\n0.47180873952099694\n\n\nAs we‚Äôd expect from our earlier tests, Sex appears to be a better split.\nTo make it easier to find the best binary split, we can create a simple interactive tool (note that this only works in Kaggle if you click ‚ÄúCopy and Edit‚Äù in the top right to open the notebook editor):\n\ndef iscore(nm, split):\n    col = trn_xs[nm]\n    return score(col, trn_y, split)\n\nfrom ipywidgets import interact\ninteract(nm=conts, split=15.5)(iscore);\n\n\n\n\nTry selecting different columns and split points using the dropdown and slider above. What splits can you find that increase the purity of the data?\nWe can do the same thing for the categorical variables:\n\ninteract(nm=cats, split=2)(iscore);\n\n\n\n\nThat works well enough, but it‚Äôs rather slow and fiddly. Perhaps we could get the computer to automatically find the best split point for a column for us? For example, to find the best split point for age we‚Äôd first need to make a list of all the possible split points (i.e all the unique values of that field)‚Ä¶:\n\nnm = \"Age\"\ncol = trn_xs[nm]\nunq = col.unique()\nunq.sort()\nunq\n\narray([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])\n\n\n‚Ä¶and find which index of those values is where score() is the lowest:\n\nscores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\nunq[scores.argmin()]\n\n6.0\n\n\nBased on this, it looks like, for instance, that for the Age column, 6 is the optimal cutoff according to our training set.\nWe can write a little function that implements this idea:\n\ndef min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx]\n\nmin_col(trn_df, \"Age\")\n\n(6.0, 0.478316717508991)\n\n\nLet‚Äôs try all the columns:\n\ncols = cats+conts\n{o:min_col(trn_df, o) for o in cols}\n\n{'Sex': (0, 0.40787530982063946),\n 'Embarked': (0, 0.47883342573147836),\n 'Age': (6.0, 0.478316717508991),\n 'SibSp': (4, 0.4783740258817434),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736597),\n 'Pclass': (2, 0.46048261885806596)}\n\n\nAccording to this, Sex&lt;=0 is the best split we can use.\nWe‚Äôve just re-invented the OneR classifier (or at least, a minor variant of it), which was found to be one of the most effective classifiers in real-world datasets, compared to the algorithms in use in 1993. Since it‚Äôs so simple and surprisingly effective, it makes for a great baseline ‚Äì that is, a starting point that you can use to compare your more sophisticated models to.\nWe found earlier that our OneR rule had an error of around 0.215, so we‚Äôll keep that in mind as we try out more sophisticated approaches."
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#creating-a-decision-tree",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#creating-a-decision-tree",
    "title": "07-How Random Forests Really Work",
    "section": "Creating a decision tree",
    "text": "Creating a decision tree\nHow can we improve our OneR classifier, which predicts survival based only on Sex?\nHow about we take each of our two groups, female and male, and create one more binary split for each of them. That is: find the single best split for females, and the single best split for males. To do this, all we have to do is repeat the previous section‚Äôs steps, once for males, and once for females.\nFirst, we‚Äôll remove Sex from the list of possible splits (since we‚Äôve already used it, and there‚Äôs only one possible split for that binary column), and create our two groups:\n\ncols.remove(\"Sex\")\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn_df[~ismale]\n\nNow let‚Äôs find the single best binary split for males‚Ä¶:\n\n{o:min_col(males, o) for o in cols}\n\n{'Embarked': (0, 0.3875581870410906),\n 'Age': (6.0, 0.3739828371010595),\n 'SibSp': (4, 0.3875864227586273),\n 'Parch': (0, 0.3874704821461959),\n 'LogFare': (2.803360380906535, 0.3804856231758151),\n 'Pclass': (1, 0.38155442004360934)}\n\n\n‚Ä¶and for females:\n\n{o:min_col(females, o) for o in cols}\n\n{'Embarked': (0, 0.4295252982857327),\n 'Age': (50.0, 0.4225927658431649),\n 'SibSp': (4, 0.42319212059713535),\n 'Parch': (3, 0.4193314500446158),\n 'LogFare': (4.256321678298823, 0.41350598332911376),\n 'Pclass': (2, 0.3335388911567601)}\n\n\nWe can see that the best next binary split for males is Age&lt;=6, and for females is Pclass&lt;=2.\nBy adding these rules, we have created a decision tree, where our model will first check whether Sex is female or male, and depending on the result will then check either the above Age or Pclass rules, as appropriate. We could then repeat the process, creating new additional rules for each of the four groups we‚Äôve now created.\nRather than writing that code manually, we can use DecisionTreeClassifier, from sklearn, which does exactly that for us:\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);\n\nOne handy feature or this class is that it provides a function for drawing a tree representing the rules:\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\ndraw_tree(m, trn_xs, size=10)\n\n\n\n\n\n\n\n\nWe can see that it‚Äôs found exactly the same splits as we did!\nIn this picture, the more orange nodes have a lower survival rate, and blue have higher survival. Each node shows how many rows (‚Äúsamples‚Äù) match that set of rules, and shows how many perish or survive (‚Äúvalues‚Äù). There‚Äôs also something called ‚Äúgini‚Äù. That‚Äôs another measure of impurity, and it‚Äôs very similar to the score() we created earlier. It‚Äôs defined as follows:\n\ndef gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2\n\nWhat this calculates is the probability that, if you pick two rows from a group, you‚Äôll get the same Survived result each time. If the group is all the same, the probability is 1.0, and 0.0 if they‚Äôre all different:\n\ngini(df.Sex=='female'), gini(df.Sex=='male')\n\n(0.3828350034484158, 0.3064437162277842)\n\n\nLet‚Äôs see how this model compares to our OneR version:\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.2242152466367713\n\n\nIt‚Äôs a tiny bit worse. Since this is such a small dataset (we‚Äôve only got around 200 rows in our validation set) this small difference isn‚Äôt really meaningful. Perhaps we‚Äôll see better results if we create a bigger tree:\n\nm = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=25)\n\n\n\n\n\n\n\n\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.18385650224215247\n\n\nIt looks like this is an improvement, although again it‚Äôs a bit hard to tell with small datasets like this. Let‚Äôs try submitting it to Kaggle:\n\ntst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes)\ntst_xs,_ = xs_y(tst_df)\n\ndef subm(preds, suff):\n    tst_df['Survived'] = preds\n    sub_df = tst_df[['PassengerId','Survived']]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\nsubm(m.predict(tst_xs), 'tree')\n\nWhen I submitted this, I got a score of 0.765, which isn‚Äôt as good as our linear models or most of our neural nets, but it‚Äôs pretty close to those results.\nHopefully you can now see why we didn‚Äôt really need to create dummy variables, but instead just converted the labels into numbers using some (potentially arbitary) ordering of categories. For instance, here‚Äôs how the first few items of Embarked are labeled:\n\ndf.Embarked.head()\n\n0    S\n1    C\n2    S\n3    S\n4    S\nName: Embarked, dtype: category\nCategories (3, object): ['C', 'Q', 'S']\n\n\n‚Ä¶resulting in these integer codes:\n\ndf.Embarked.cat.codes.head()\n\n0    2\n1    0\n2    2\n3    2\n4    2\ndtype: int8\n\n\nSo let‚Äôs say we wanted to split into ‚ÄúC‚Äù in one group, vs ‚ÄúQ‚Äù or ‚ÄúS‚Äù in the other group. Then we just have to split on codes &lt;=0 (since C is mapped to category 0). Note that if we wanted to split into ‚ÄúQ‚Äù in one group, we‚Äôd need to use two binary splits, first to separate ‚ÄúC‚Äù from ‚ÄúQ‚Äù and ‚ÄúS‚Äù, and then a second split to separate ‚ÄúQ‚Äù from ‚ÄúS‚Äù. For this reason, sometimes it can still be helpful to use dummy variables for categorical variables with few levels (like this one).\nIn practice, I often use dummy variables for &lt;4 levels, and numeric codes for &gt;=4 levels."
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#the-random-forest",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#the-random-forest",
    "title": "07-How Random Forests Really Work",
    "section": "The random forest",
    "text": "The random forest\nWe can‚Äôt make the decision tree much bigger than the example above, since some leaf nodes already have only 50 rows in them. That‚Äôs not a lot of data to make a prediction.\nSo how could we use bigger trees? One big insight came from Leo Breiman: what if we create lots of bigger trees, and take the average of their predictions? Taking the average prediction of a bunch of models in this way is known as bagging.\nThe idea is that we want each model‚Äôs predictions in the averaged ensemble to be uncorrelated with each other model. That way, if we average the predictions, the average will be equal to the true target value ‚Äì that‚Äôs because the average of lots of uncorrelated random errors is zero. That‚Äôs quite an amazing insight!\nOne way we can create a bunch of uncorrelated models is to train each of them on a different random subset of the data. Here‚Äôs how we can create a tree on a random subset of the data:\n\ndef get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])\n\nNow we can create as many trees as we want:\n\ntrees = [get_tree() for t in range(100)]\n\nOur prediction will be the average of these trees‚Äô predictions:\n\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\nmean_absolute_error(val_y, avg_probs)\n\n0.2272645739910314\n\n\nThis is nearly identical to what sklearn‚Äôs RandomForestClassifier does. The main extra piece in a ‚Äúreal‚Äù random forest is that as well as choosing a random sample of data for each tree, it also picks a random subset of columns for each split. Here‚Äôs how we repeat the above process with a random forest:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y);\nmean_absolute_error(val_y, rf.predict(val_xs))\n\n0.18834080717488788\n\n\nWe can submit that to Kaggle too:\n\nsubm(rf.predict(tst_xs), 'rf')\n\nI found that gave nearly an identical result as our single tree (which, in turn, was slightly lower than our linear and neural net models in the previous notebook).\nOne particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using feature_importances_:\n\npd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');\n\n\n\n\n\n\n\n\nWe can see that Sex is by far the most important predictor, with Pclass a distant second, and LogFare and Age behind that. In datasets with many columns, I generally recommend creating a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn‚Äôt really need to take the log() of Fare, since random forests only care about order, and log() doesn‚Äôt change the order ‚Äì we only did it to make our graphs earlier easier to read.)\nFor details about deriving and understanding feature importances, and the many other important diagnostic tools provided by random forests, take a look at chapter 8 of our book."
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#conclusion",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#conclusion",
    "title": "07-How Random Forests Really Work",
    "section": "Conclusion",
    "text": "Conclusion\nSo what can we take away from all this?\nI think the first thing I‚Äôd note from this is that, clearly, more complex models aren‚Äôt always better. Our ‚ÄúOneR‚Äù model, consisting of a single binary split, was nearly as good as our more complex models. Perhaps in practice a simple model like this might be much easier to use, and could be worth considering. Our random forest wasn‚Äôt an improvement on the single decision tree at all.\nSo we should always be careful to benchmark simple models, as see if they‚Äôre good enough for our needs. In practice, you will often find that simple models will have trouble providing adequate accuracy for more complex tasks, such as recommendation systems, NLP, computer vision, or multivariate time series. But there‚Äôs no need to guess ‚Äì it‚Äôs so easy to try a few different models, there‚Äôs no reason not to give the simpler ones a go too!\nAnother thing I think we can take away is that random forests aren‚Äôt actually that complicated at all. We were able to implement the key features of them in a notebook quite quickly. And they aren‚Äôt sensitive to issues like normalization, interactions, or non-linear transformations, which make them extremely easy to work with, and hard to mess up!\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you‚Äôre looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won‚Äôt get counted!) And if you have any questions or comments, please pop them below ‚Äì I read every comment I receive!"
  }
]