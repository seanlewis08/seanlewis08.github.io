[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html",
    "title": "Introduction",
    "section": "",
    "text": "Previously I’ve shown how to create a linear model and neural net from scratch, and used it to create a solid submission to Kaggle’s Titanic competition. However, for tabular data (i.e data that looks like spreadsheet or database tables, such as the data for the Titanic competition) it’s more common to see good results by using ensembles of decision trees, such as Random Forests and Gradient Boosting Machines.\nIn this notebook, we’re going to learn all about Random Forests, by building one from scratch, and using it to submit to the Titanic competition! That might sound like a pretty big stretch, but I think you’ll be surprised to discover how straightforward it actually is.\nWe’ll start by importing the basic set of libraries we normally need for data science work, and setting numpy to use our display space more efficiently:\n::: {#cell-3 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2022-05-23T04:37:24.640765Z”,“iopub.status.busy”:“2022-05-23T04:37:24.640339Z”,“iopub.status.idle”:“2022-05-23T04:37:25.174055Z”,“shell.execute_reply”:“2022-05-23T04:37:25.172992Z”,“shell.execute_reply.started”:“2022-05-23T04:37:24.640663Z”}’ execution_count=1}\n:::"
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#data-preprocessing",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#data-preprocessing",
    "title": "Introduction",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nWe’ll create DataFrames from the CSV files just like we did in the “linear model and neural net from scratch” notebook, and do much the same preprocessing (so go back and check that out if you’re not already familiar with the dataset):\n\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle: path = Path('../input/titanic')\nelse:\n    import zipfile,kaggle\n    path = Path('titanic')\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\nmodes = df.mode().iloc[0]\n\ntitanic.zip: Skipping, found more recently modified local copy (use --force to force download)\n\n\nOne difference with Random Forests however is that we don’t generally have to create dummy variables like we did for non-numeric columns in the linear models and neural network. Instead, we can just convert those fields to categorical variables, which internally in Pandas makes a list of all the unique values in the column, and replaces each value with a number. The number is just an index for looking up the value in the list of all unique values.\n\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True)\n    df['LogFare'] = np.log1p(df['Fare'])\n    df['Embarked'] = pd.Categorical(df.Embarked)\n    df['Sex'] = pd.Categorical(df.Sex)\n\nproc_data(df)\nproc_data(tst_df)\n\nWe’ll make a list of the continuous, categorical, and dependent variables. Note that we no longer consider Pclass a categorical variable. That’s because it’s ordered (i.e 1st, 2nd, and 3rd class have an order), and decision trees, as we’ll see, only care about order, not about absolute value.\n\ncats=[\"Sex\",\"Embarked\"]\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\ndep=\"Survived\"\n\nEven although we’ve made the cats columns categorical, they are still shown by Pandas as their original values:\n\ndf.Sex.head()\n\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']\n\n\nHowever behind the scenes they’re now stored as integers, with indices that are looked up in the Categories list shown in the output above. We can view the stored values by looking in the cat.codes attribute:\n\ndf.Sex.cat.codes.head()\n\n0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8"
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#binary-splits",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#binary-splits",
    "title": "Introduction",
    "section": "Binary splits",
    "text": "Binary splits\nBefore we create a Random Forest or Gradient Boosting Machine, we’ll first need to learn how to create a decision tree, from which both of these models are built.\nAnd to create a decision tree, we’ll first need to create a binary split, since that’s what a decision tree is built from.\nA binary split is where all rows are placed into one of two groups, based on whether they’re above or below some threshold of some column. For example, we could split the rows of our dataset into males and females, by using the threshold 0.5 and the column Sex (since the values in the column are 0 for female and 1 for male). We can use a plot to see how that would split up our data – we’ll use the Seaborn library, which is a layer on top of matplotlib that makes some useful charts easier to create, and more aesthetically pleasing by default:\n\nimport seaborn as sns\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");\n\n\n\n\n\n\n\n\nHere we see that (on the left) if we split the data into males and females, we’d have groups that have very different survival rates: &gt;70% for females, and &lt;20% for males. We can also see (on the right) that the split would be reasonably even, with over 300 passengers (out of around 900) in each group.\nWe could create a very simple “model” which simply says that all females survive, and no males do. To do so, we better first split our data into a training and validation set, to see how accurate this approach turns out to be:\n\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df, test_size=0.25)\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n\n(In the previous step we also replaced the categorical variables with their integer codes, since some of the models we’ll be building in a moment require that.)\nNow we can create our independent variables (the x variables) and dependent (the y variable):\n\ndef xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n\nHere’s the predictions for our extremely simple model, where female is coded as 0:\n\npreds = val_xs.Sex==0\n\nWe’ll use mean absolute error to measure how good this model is:\n\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)\n\n0.21524663677130046\n\n\nAlternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work – here’s an example of how we could look at LogFare:\n\ndf_fare = trn_df[trn_df.LogFare&gt;0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n\n\n\n\n\n\n\n\nThe boxenplot above shows quantiles of LogFare for each group of Survived==0 and Survived==1. It shows that the average LogFare for passengers that didn’t survive is around 2.5, and for those that did it’s around 3.2. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\nLet’s create a simple model based on this observation:\n\npreds = val_xs.LogFare&gt;2.7\n\n…and test it out:\n\nmean_absolute_error(val_y, preds)\n\n0.336322869955157\n\n\nThis is quite a bit less accurate than our model that used Sex as the single binary split.\nIdeally, we’d like some way to try more columns and breakpoints more easily. We could create a function that returns how good our model is, in order to more quickly try out a few different splits. We’ll create a score function to do this. Instead of returning the mean absolute error, we’ll calculate a measure of impurity – that is, how much the binary split creates two groups where the rows in a group are each similar to each other, or dissimilar.\nWe can measure the similarity of rows inside a group by taking the standard deviation of the dependent variable. If it’s higher, then it means the rows are more different to each other. We’ll then multiply this by the number of rows, since a bigger group has more impact than a smaller group:\n\ndef _side_score(side, y):\n    tot = side.sum()\n    if tot&lt;=1: return 0\n    return y[side].std()*tot\n\nNow we’ve got that written, we can calculate the score for a split by adding up the scores for the “left hand side” (lhs) and “right hand side” (rhs):\n\n    \ndef score(col, y, split):\n    lhs = col&lt;=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)\n\nFor instance, here’s the impurity score for the split on Sex:\n\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n\n0.40787530982063946\n\n\n…and for LogFare:\n\nscore(trn_xs[\"LogFare\"], trn_y, 2.7)\n\n0.47180873952099694\n\n\nAs we’d expect from our earlier tests, Sex appears to be a better split.\nTo make it easier to find the best binary split, we can create a simple interactive tool (note that this only works in Kaggle if you click “Copy and Edit” in the top right to open the notebook editor):\n\ndef iscore(nm, split):\n    col = trn_xs[nm]\n    return score(col, trn_y, split)\n\nfrom ipywidgets import interact\ninteract(nm=conts, split=15.5)(iscore);\n\n\n\n\nTry selecting different columns and split points using the dropdown and slider above. What splits can you find that increase the purity of the data?\nWe can do the same thing for the categorical variables:\n\ninteract(nm=cats, split=2)(iscore);\n\n\n\n\nThat works well enough, but it’s rather slow and fiddly. Perhaps we could get the computer to automatically find the best split point for a column for us? For example, to find the best split point for age we’d first need to make a list of all the possible split points (i.e all the unique values of that field)…:\n\nnm = \"Age\"\ncol = trn_xs[nm]\nunq = col.unique()\nunq.sort()\nunq\n\narray([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])\n\n\n…and find which index of those values is where score() is the lowest:\n\nscores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\nunq[scores.argmin()]\n\n6.0\n\n\nBased on this, it looks like, for instance, that for the Age column, 6 is the optimal cutoff according to our training set.\nWe can write a little function that implements this idea:\n\ndef min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx]\n\nmin_col(trn_df, \"Age\")\n\n(6.0, 0.478316717508991)\n\n\nLet’s try all the columns:\n\ncols = cats+conts\n{o:min_col(trn_df, o) for o in cols}\n\n{'Sex': (0, 0.40787530982063946),\n 'Embarked': (0, 0.47883342573147836),\n 'Age': (6.0, 0.478316717508991),\n 'SibSp': (4, 0.4783740258817434),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736597),\n 'Pclass': (2, 0.46048261885806596)}\n\n\nAccording to this, Sex&lt;=0 is the best split we can use.\nWe’ve just re-invented the OneR classifier (or at least, a minor variant of it), which was found to be one of the most effective classifiers in real-world datasets, compared to the algorithms in use in 1993. Since it’s so simple and surprisingly effective, it makes for a great baseline – that is, a starting point that you can use to compare your more sophisticated models to.\nWe found earlier that our OneR rule had an error of around 0.215, so we’ll keep that in mind as we try out more sophisticated approaches."
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#creating-a-decision-tree",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#creating-a-decision-tree",
    "title": "Introduction",
    "section": "Creating a decision tree",
    "text": "Creating a decision tree\nHow can we improve our OneR classifier, which predicts survival based only on Sex?\nHow about we take each of our two groups, female and male, and create one more binary split for each of them. That is: find the single best split for females, and the single best split for males. To do this, all we have to do is repeat the previous section’s steps, once for males, and once for females.\nFirst, we’ll remove Sex from the list of possible splits (since we’ve already used it, and there’s only one possible split for that binary column), and create our two groups:\n\ncols.remove(\"Sex\")\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn_df[~ismale]\n\nNow let’s find the single best binary split for males…:\n\n{o:min_col(males, o) for o in cols}\n\n{'Embarked': (0, 0.3875581870410906),\n 'Age': (6.0, 0.3739828371010595),\n 'SibSp': (4, 0.3875864227586273),\n 'Parch': (0, 0.3874704821461959),\n 'LogFare': (2.803360380906535, 0.3804856231758151),\n 'Pclass': (1, 0.38155442004360934)}\n\n\n…and for females:\n\n{o:min_col(females, o) for o in cols}\n\n{'Embarked': (0, 0.4295252982857327),\n 'Age': (50.0, 0.4225927658431649),\n 'SibSp': (4, 0.42319212059713535),\n 'Parch': (3, 0.4193314500446158),\n 'LogFare': (4.256321678298823, 0.41350598332911376),\n 'Pclass': (2, 0.3335388911567601)}\n\n\nWe can see that the best next binary split for males is Age&lt;=6, and for females is Pclass&lt;=2.\nBy adding these rules, we have created a decision tree, where our model will first check whether Sex is female or male, and depending on the result will then check either the above Age or Pclass rules, as appropriate. We could then repeat the process, creating new additional rules for each of the four groups we’ve now created.\nRather than writing that code manually, we can use DecisionTreeClassifier, from sklearn, which does exactly that for us:\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);\n\nOne handy feature or this class is that it provides a function for drawing a tree representing the rules:\n\nimport graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\ndraw_tree(m, trn_xs, size=10)\n\n\n\n\n\n\n\n\nWe can see that it’s found exactly the same splits as we did!\nIn this picture, the more orange nodes have a lower survival rate, and blue have higher survival. Each node shows how many rows (“samples”) match that set of rules, and shows how many perish or survive (“values”). There’s also something called “gini”. That’s another measure of impurity, and it’s very similar to the score() we created earlier. It’s defined as follows:\n\ndef gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2\n\nWhat this calculates is the probability that, if you pick two rows from a group, you’ll get the same Survived result each time. If the group is all the same, the probability is 1.0, and 0.0 if they’re all different:\n\ngini(df.Sex=='female'), gini(df.Sex=='male')\n\n(0.3828350034484158, 0.3064437162277842)\n\n\nLet’s see how this model compares to our OneR version:\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.2242152466367713\n\n\nIt’s a tiny bit worse. Since this is such a small dataset (we’ve only got around 200 rows in our validation set) this small difference isn’t really meaningful. Perhaps we’ll see better results if we create a bigger tree:\n\nm = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=25)\n\n\n\n\n\n\n\n\n\nmean_absolute_error(val_y, m.predict(val_xs))\n\n0.18385650224215247\n\n\nIt looks like this is an improvement, although again it’s a bit hard to tell with small datasets like this. Let’s try submitting it to Kaggle:\n\ntst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes)\ntst_xs,_ = xs_y(tst_df)\n\ndef subm(preds, suff):\n    tst_df['Survived'] = preds\n    sub_df = tst_df[['PassengerId','Survived']]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\nsubm(m.predict(tst_xs), 'tree')\n\nWhen I submitted this, I got a score of 0.765, which isn’t as good as our linear models or most of our neural nets, but it’s pretty close to those results.\nHopefully you can now see why we didn’t really need to create dummy variables, but instead just converted the labels into numbers using some (potentially arbitary) ordering of categories. For instance, here’s how the first few items of Embarked are labeled:\n\ndf.Embarked.head()\n\n0    S\n1    C\n2    S\n3    S\n4    S\nName: Embarked, dtype: category\nCategories (3, object): ['C', 'Q', 'S']\n\n\n…resulting in these integer codes:\n\ndf.Embarked.cat.codes.head()\n\n0    2\n1    0\n2    2\n3    2\n4    2\ndtype: int8\n\n\nSo let’s say we wanted to split into “C” in one group, vs “Q” or “S” in the other group. Then we just have to split on codes &lt;=0 (since C is mapped to category 0). Note that if we wanted to split into “Q” in one group, we’d need to use two binary splits, first to separate “C” from “Q” and “S”, and then a second split to separate “Q” from “S”. For this reason, sometimes it can still be helpful to use dummy variables for categorical variables with few levels (like this one).\nIn practice, I often use dummy variables for &lt;4 levels, and numeric codes for &gt;=4 levels."
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#the-random-forest",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#the-random-forest",
    "title": "Introduction",
    "section": "The random forest",
    "text": "The random forest\nWe can’t make the decision tree much bigger than the example above, since some leaf nodes already have only 50 rows in them. That’s not a lot of data to make a prediction.\nSo how could we use bigger trees? One big insight came from Leo Breiman: what if we create lots of bigger trees, and take the average of their predictions? Taking the average prediction of a bunch of models in this way is known as bagging.\nThe idea is that we want each model’s predictions in the averaged ensemble to be uncorrelated with each other model. That way, if we average the predictions, the average will be equal to the true target value – that’s because the average of lots of uncorrelated random errors is zero. That’s quite an amazing insight!\nOne way we can create a bunch of uncorrelated models is to train each of them on a different random subset of the data. Here’s how we can create a tree on a random subset of the data:\n\ndef get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])\n\nNow we can create as many trees as we want:\n\ntrees = [get_tree() for t in range(100)]\n\nOur prediction will be the average of these trees’ predictions:\n\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\nmean_absolute_error(val_y, avg_probs)\n\n0.2272645739910314\n\n\nThis is nearly identical to what sklearn’s RandomForestClassifier does. The main extra piece in a “real” random forest is that as well as choosing a random sample of data for each tree, it also picks a random subset of columns for each split. Here’s how we repeat the above process with a random forest:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y);\nmean_absolute_error(val_y, rf.predict(val_xs))\n\n0.18834080717488788\n\n\nWe can submit that to Kaggle too:\n\nsubm(rf.predict(tst_xs), 'rf')\n\nI found that gave nearly an identical result as our single tree (which, in turn, was slightly lower than our linear and neural net models in the previous notebook).\nOne particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using feature_importances_:\n\npd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');\n\n\n\n\n\n\n\n\nWe can see that Sex is by far the most important predictor, with Pclass a distant second, and LogFare and Age behind that. In datasets with many columns, I generally recommend creating a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn’t really need to take the log() of Fare, since random forests only care about order, and log() doesn’t change the order – we only did it to make our graphs earlier easier to read.)\nFor details about deriving and understanding feature importances, and the many other important diagnostic tools provided by random forests, take a look at chapter 8 of our book."
  },
  {
    "objectID": "posts/from_scratch_model/07-how-random-forests-really-work.html#conclusion",
    "href": "posts/from_scratch_model/07-how-random-forests-really-work.html#conclusion",
    "title": "Introduction",
    "section": "Conclusion",
    "text": "Conclusion\nSo what can we take away from all this?\nI think the first thing I’d note from this is that, clearly, more complex models aren’t always better. Our “OneR” model, consisting of a single binary split, was nearly as good as our more complex models. Perhaps in practice a simple model like this might be much easier to use, and could be worth considering. Our random forest wasn’t an improvement on the single decision tree at all.\nSo we should always be careful to benchmark simple models, as see if they’re good enough for our needs. In practice, you will often find that simple models will have trouble providing adequate accuracy for more complex tasks, such as recommendation systems, NLP, computer vision, or multivariate time series. But there’s no need to guess – it’s so easy to try a few different models, there’s no reason not to give the simpler ones a go too!\nAnother thing I think we can take away is that random forests aren’t actually that complicated at all. We were able to implement the key features of them in a notebook quite quickly. And they aren’t sensitive to issues like normalization, interactions, or non-linear transformations, which make them extremely easy to work with, and hard to mess up!\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you’re looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won’t get counted!) And if you have any questions or comments, please pop them below – I read every comment I receive!"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html",
    "title": "My Notebook 2",
    "section": "",
    "text": "In this notebook we’re going to build and train a deep learning model “from scratch” – by which I mean that we’re not going to use any pre-built architecture, or optimizers, or data loading frameworks, etc.\nWe’ll be assuming you already know the basics of how a neural network works. If you don’t, read this notebook first: How does a neural net really work?. We’ll be using Kaggle’s Titanic competition in this notebook, because it’s very small and simple, but also has displays many of the tricky real-life issues that we need to handle in most practical projects. (Note, however, that this competition is a small “learner” competition on Kaggle, so don’t expect to actually see much benefits from using a neural net just yet; that will come once we try our some real competitions!)\nIt’s great to be able to run the same notebook on your own machine or Colab, as well as Kaggle. To allow for this, we use this code to download the data as needed when not on Kaggle (see this notebook for details about this technique):\n\nimport os\nfrom pathlib import Path\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path('../input/titanic')\nelse:\n    path = Path('titanic')\n    if not path.exists():\n        import zipfile,kaggle\n        kaggle.api.competition_download_cli(str(path))\n        zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nNote that the data for Kaggle comps always lives in the ../input folder. The easiest way to get the path is to click the “K” button in the top-right of the Kaggle notebook, click on the folder shown there, and click the copy button.\nWe’ll be using numpy and pytorch for array calculations in this notebook, and pandas for working with tabular data, so we’ll import them and set them to display using a bit more space than they default to.\n\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#introduction",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#introduction",
    "title": "My Notebook 2",
    "section": "",
    "text": "In this notebook we’re going to build and train a deep learning model “from scratch” – by which I mean that we’re not going to use any pre-built architecture, or optimizers, or data loading frameworks, etc.\nWe’ll be assuming you already know the basics of how a neural network works. If you don’t, read this notebook first: How does a neural net really work?. We’ll be using Kaggle’s Titanic competition in this notebook, because it’s very small and simple, but also has displays many of the tricky real-life issues that we need to handle in most practical projects. (Note, however, that this competition is a small “learner” competition on Kaggle, so don’t expect to actually see much benefits from using a neural net just yet; that will come once we try our some real competitions!)\nIt’s great to be able to run the same notebook on your own machine or Colab, as well as Kaggle. To allow for this, we use this code to download the data as needed when not on Kaggle (see this notebook for details about this technique):\n\nimport os\nfrom pathlib import Path\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle: path = Path('../input/titanic')\nelse:\n    path = Path('titanic')\n    if not path.exists():\n        import zipfile,kaggle\n        kaggle.api.competition_download_cli(str(path))\n        zipfile.ZipFile(f'{path}.zip').extractall(path)\n\nNote that the data for Kaggle comps always lives in the ../input folder. The easiest way to get the path is to click the “K” button in the top-right of the Kaggle notebook, click on the folder shown there, and click the copy button.\nWe’ll be using numpy and pytorch for array calculations in this notebook, and pandas for working with tabular data, so we’ll import them and set them to display using a bit more space than they default to.\n\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#cleaning-the-data",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#cleaning-the-data",
    "title": "My Notebook 2",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nThis is a tabular data competition – the data is in the form of a table. It’s provided as a Comma Separated Values (CSV) file. We can open it using the pandas library, which will create a DataFrame.\n\ndf = pd.read_csv(path/'train.csv')\ndf\n\nAs we learned in the How does a neural net really work notebook, we going to want to multiply each column by some coefficients. But we can see in the Cabin column that there are NaN values, which is how Pandas refers to missing values. We can’t multiply something by a missing value!\nLet’s check which columns contain NaN values. Pandas’ isna() function returns True (which is treated as 1 when used as a number) for NaN values, so we can just add them up for each column:\n\ndf.isna().sum()\n\nNotice that by default Pandas sums over columns.\nWe’ll need to replace the missing values with something. It doesn’t generally matter too much what we choose. We’ll use the most common value (the “mode”). We can use the mode function for that. One wrinkle is that it returns more than one row in the case of ties, so we just grab the first row with iloc[0]:\n\nmodes = df.mode().iloc[0]\nmodes\n\nBTW, it’s never a good idea to use functions without understanding them. So be sure to google for anything you’re not familiar with. E.g if you want to learn about iloc (which is a very important function indeed!) then Google will give you a link to a great tutorial.\nNow that we’ve got the mode of each column, we can use fillna to replace the missing values with the mode of each column. We’ll do it “in place” – meaning that we’ll change the dataframe itself, rather than returning a new one.\n\ndf.fillna(modes, inplace=True)\n\nWe can now check there’s no missing values left:\n\ndf.isna().sum()\n\nHere’s how we get a quick summary of all the numeric columns in the dataset:\n\nimport numpy as np\n\ndf.describe(include=(np.number))\n\nWe can see that Fare contains mainly values of around 0 to 30, but there’s a few really big ones. This is very common with fields contain monetary values, and it can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will dominate the result.\nYou can see the issue most clearly visually by looking at a histogram, which shows a long tail to the right (and don’t forget: if you’re not entirely sure what a histogram is, Google “histogram tutorial” and do a bit of reading before continuing on):\n\ndf['Fare'].hist();\n\nTo fix this, the most common approach is to take the logarithm, which squishes the big numbers and makes the distribution more reasonable. Note, however, that there are zeros in the Fare column, and log(0) is infinite – to fix this, we’ll simply add 1 to all values first:\n\ndf['LogFare'] = np.log(df['Fare']+1)\n\nThe histogram now shows a more even distribution of values without the long tail:\n\ndf['LogFare'].hist();\n\nIt looks from the describe() output like Pclass contains just 3 values, which we can confirm by looking at the Data Dictionary (which you should always study carefully for any project!) –\n\npclasses = sorted(df.Pclass.unique())\npclasses\n\nHere’s how we get a quick summary of all the non-numeric columns in the dataset:\n\ndf.describe(include=[object])\n\nClearly we can’t multiply strings like male or S by coefficients, so we need to replace those with numbers.\nWe do that by creating new columns containing dummy variables. A dummy variable is a column that contains a 1 where a particular column contains a particular value, or a 0 otherwise. For instance, we could create a dummy variable for Sex='male', which would be a new column containing 1 for rows where Sex is 'male', and 0 for rows where it isn’t.\nPandas can create these automatically using get_dummies, which also remove the original columns. We’ll create dummy variables for Pclass, even although it’s numeric, since the numbers 1, 2, and 3 correspond to first, second, and third class cabins - not to counts or measures that make sense to multiply by. We’ll also create dummies for Sex and Embarked since we’ll want to use those as predictors in our model. On the other hand, Cabin, Name, and Ticket have too many unique values for it to make sense creating dummy variables for them.\n\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ndf.columns\n\nWe can see that 5 columns have been added to the end – one for each of the possible values of each of the three columns we requested, and that those three requested columns have been removed.\nHere’s what the first few rows of those newly added columns look like:\n\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ndf[added_cols].head()\n\nNow we can create our independent (predictors) and dependent (target) variables. They both need to be PyTorch tensors. Our dependent variable is Survived:\n\nfrom torch import tensor\n\nt_dep = tensor(df.Survived)\n\nOur independent variables are all the continuous variables of interest plus all the dummy variables we just created:\n\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep\n\nHere’s the number of rows and columns we have for our independent variables:\n\nt_indep.shape"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#setting-up-a-linear-model",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#setting-up-a-linear-model",
    "title": "My Notebook 2",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nNow that we’ve got a matrix of independent variables and a dependent variable vector, we can work on calculating our predictions and our loss. In this section, we’re going to manually do a single step of calculating predictions and loss for every row of our data.\nOur first model will be a simple linear model. We’ll need a coefficient for each column in t_indep. We’ll pick random numbers in the range (-0.5,0.5), and set our manual seed so that my explanations in the prose in this notebook will be consistent with what you see when you run it.\n\ntorch.manual_seed(442)\n\nn_coeff = t_indep.shape[1]\ncoeffs = torch.rand(n_coeff)-0.5\ncoeffs\n\nOur predictions will be calculated by multiplying each row by the coefficients, and adding them up. One interesting point here is that we don’t need a separate constant term (also known as a “bias” or “intercept” term), or a column of all 1s to give the same effect has having a constant term. That’s because our dummy variables already cover the entire dataset – e.g. there’s a column for “male” and a column for “female”, and everyone in the dataset is in exactly one of these; therefore, we don’t need a separate intercept term to cover rows that aren’t otherwise part of a column.\nHere’s what the multiplication looks like:\n\nt_indep*coeffs\n\nWe can see we’ve got a problem here. The sums of each row will be dominated by the first column, which is Age, since that’s bigger on average than all the others.\nLet’s make all the columns contain numbers from 0 to 1, by dividing each column by its max():\n\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\n\nAs we see, that removes the problem of one column dominating all the others:\n\nt_indep*coeffs\n\nOne thing you hopefully noticed is how amazingly cool this line of code is:\nt_indep = t_indep / vals\nThat is dividing a matrix by a vector – what on earth does that mean?!? The trick here is that we’re taking advantage of a technique in numpy and PyTorch (and many other languages, going all the way back to APL) called broadcasting. In short, this acts as if there’s a separate copy of the vector for every row of the matrix, so it divides each row of the matrix by the vector. In practice, it doesn’t actually make any copies, and does the whole thing in a highly optimized way, taking full advantage of modern CPUs (or, indeed, GPUs, if we’re using them). Broadcasting is one of the most important techniques for making your code concise, maintainable, and fast, so it’s well worth studying and practicing.\nWe can now create predictions from our linear model, by adding up the rows of the product:\n\npreds = (t_indep*coeffs).sum(axis=1)\n\nLet’s take a look at the first few:\n\npreds[:10]\n\nOf course, these predictions aren’t going to be any use, since our coefficients are random – they’re just a starting point for our gradient descent process.\nTo do gradient descent, we need a loss function. Taking the average error of the rows (i.e. the absolute value of the difference between the prediction and the dependent) is generally a reasonable approach:\n\nloss = torch.abs(preds-t_dep).mean()\nloss\n\nNow that we’ve tested out a way of calculating predictions, and loss, let’s pop them into functions to make life easier:\n\ndef calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#doing-a-gradient-descent-step",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#doing-a-gradient-descent-step",
    "title": "My Notebook 2",
    "section": "Doing a gradient descent step",
    "text": "Doing a gradient descent step\nIn this section, we’re going to do a single “epoch” of gradient descent manually. The only thing we’re going to automate is calculating gradients, because let’s face it that’s pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we’ll need to call requires_grad_() on our coeffs (if you’re not sure why, review the previous notebook, How does a neural net really work?, before continuing):\n\ncoeffs.requires_grad_()\n\nNow when we calculate our loss, PyTorch will keep track of all the steps, so we’ll be able to get the gradients afterwards:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss\n\nUse backward() to ask PyTorch to calculate gradients now:\n\nloss.backward()\n\nLet’s see what they look like:\n\ncoeffs.grad\n\nNote that each time we call backward, the gradients are actually added to whatever is in the .grad attribute. Let’s try running the above steps again:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\ncoeffs.grad\n\nAs you see, our .grad values are have doubled. That’s because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero.\nWe can now do one gradient descent step, and check that our loss decreases:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)\n    coeffs.grad.zero_()\n    print(calc_loss(coeffs, t_indep, t_dep))\n\nNote that a.sub_(b) subtracts b from a in-place. In PyTorch, any method that ends in _ changes its object in-place. Similarly, a.zero_() sets all elements of a tensor to zero."
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#training-the-linear-model",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#training-the-linear-model",
    "title": "My Notebook 2",
    "section": "Training the linear model",
    "text": "Training the linear model\nBefore we begin training our model, we’ll need to ensure that we hold out a validation set for calculating our metrics (for details on this, see “Getting started with NLP for absolute beginners”.\nThere’s lots of different ways we can do this. In the next notebook we’ll be comparing our approach here to what the fastai library does, so we’ll want to ensure we split the data in the same way. So let’s use RandomSplitter to get indices that will split our data into training and validation sets:\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\nNow we can apply those indicies to our independent and dependent variables:\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\nWe’ll create functions for the three things we did manually above: updating coeffs, doing one full gradient descent step, and initilising coeffs to random numbers:\n\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad(): update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\n\ndef init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n\nWe can now use these functions to train our model:\n\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nLet’s try it. Our loss will print at the end of every step, so we hope we’ll see it going down:\n\ncoeffs = train_model(18, lr=0.2)\n\nIt does!\nLet’s take a look at the coefficients for each column:\n\ndef show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\nshow_coeffs()"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#measuring-accuracy",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#measuring-accuracy",
    "title": "My Notebook 2",
    "section": "Measuring accuracy",
    "text": "Measuring accuracy\nThe Kaggle competition is not, however, scored by absolute error (which is our loss function). It’s scored by accuracy – the proportion of rows where we correctly predict survival. Let’s see how accurate we were on the validation set. First, calculate the predictions:\n\npreds = calc_preds(coeffs, val_indep)\n\nWe’ll assume that any passenger with a score of over 0.5 is predicted to survive. So that means we’re correct for each row where preds&gt;0.5 is the same as the dependent variable:\n\nresults = val_dep.bool()==(preds&gt;0.5)\nresults[:16]\n\nLet’s see what our average accuracy is:\n\nresults.float().mean()\n\nThat’s not a bad start at all! We’ll create a function so we can calcuate the accuracy easy for other models we train:\n\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)&gt;0.5)).float().mean()\nacc(coeffs)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#using-sigmoid",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#using-sigmoid",
    "title": "My Notebook 2",
    "section": "Using sigmoid",
    "text": "Using sigmoid\nLooking at our predictions, there’s one obvious problem – some of our predictions of the probability of survival are &gt;1, and some are &lt;0:\n\npreds[:28]\n\nTo fix this, we should pass every prediction through the sigmoid function, which has a minimum at zero and maximum at one, and is defined as follows:\n\nimport sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));\n\nPyTorch already defines that function for us, so we can modify calc_preds to use it:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))\n\nLet’s train a new model now, using this updated function to calculate predictions:\n\ncoeffs = train_model(lr=100)\n\nThe loss has improved by a lot. Let’s check the accuracy:\n\nacc(coeffs)\n\nThat’s improved too! Here’s the coefficients of our trained model:\n\nshow_coeffs()\n\nThese coefficients seem reasonable – in general, older people and males were less likely to survive, and first class passengers were more likely to survive."
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#submitting-to-kaggle",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#submitting-to-kaggle",
    "title": "My Notebook 2",
    "section": "Submitting to Kaggle",
    "text": "Submitting to Kaggle\nNow that we’ve got a trained model, we can prepare a submission to Kaggle. To do that, first we need to read the test set:\n\ntst_df = pd.read_csv(path/'test.csv')\n\nIn this case, it turns out that the test set is missing Fare for one passenger. We’ll just fill it with 0 to avoid problems:\n\ntst_df['Fare'] = tst_df.Fare.fillna(0)\n\nNow we can just copy the same steps we did to our training set and do the same exact things on our test set to preprocess the data:\n\ntst_df.fillna(modes, inplace=True)\ntst_df['LogFare'] = np.log(tst_df['Fare']+1)\ntst_df = pd.get_dummies(tst_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n\ntst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)\ntst_indep = tst_indep / vals\n\nLet’s calculate our predictions of which passengers survived in the test set:\n\ntst_df['Survived'] = (calc_preds(tst_indep, coeffs)&gt;0.5).int()\n\nThe sample submission on the Kaggle competition site shows that we’re expected to upload a CSV with just PassengerId and Survived, so let’s create that and save it:\n\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df.to_csv('sub.csv', index=False)\n\nWe can check the first few rows of the file to make sure it looks reasonable:\n\n!head sub.csv\n\nWhen you click “save version” in Kaggle, and wait for the notebook to run, you’ll see that sub.csv appears in the “Data” tab. Clicking on that file will show a Submit button, which allows you to submit to the competition."
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#using-matrix-product",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#using-matrix-product",
    "title": "My Notebook 2",
    "section": "Using matrix product",
    "text": "Using matrix product\nWe can make things quite a bit neater…\nTake a look at the inner-most calculation we’re doing to get the predictions:\n\n(val_indep*coeffs).sum(axis=1)\n\nMultiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the @ operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so:\n\nval_indep@coeffs\n\nIt also turns out that this is much faster, because matrix products in PyTorch are very highly optimised.\nLet’s use this to replace how calc_preds works:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)\n\nIn order to do matrix-matrix products (which we’ll need in the next section), we need to turn coeffs into a column vector (i.e. a matrix with a single column), which we can do by passing a second argument 1 to torch.rand(), indicating that we want our coefficients to have one column:\n\ndef init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\n\nWe’ll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position:\n\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\nWe can now train our model as before and confirm we get identical outputs…:\n\ncoeffs = train_model(lr=100)\n\n…and identical accuracy:\n\nacc(coeffs)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#a-neural-network",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#a-neural-network",
    "title": "My Notebook 2",
    "section": "A neural network",
    "text": "A neural network\nWe’ve now got what we need to implement our neural network.\nFirst, we’ll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like – a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We’ll divide these coefficients by n_hidden so that when we sum them up in the next layer we’ll end up with similar magnitude numbers to what we started with.\nThen our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added.\n\ndef init_coeffs(n_hidden=20):\n    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n    layer2 = torch.rand(n_hidden, 1)-0.3\n    const = torch.rand(1)[0]\n    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()\n\nNow we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that’s our non-linearity), and the second is passed to torch.sigmoid as before.\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    l1,l2,const = coeffs\n    res = F.relu(indeps@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res)\n\nFinally, now that we have more than one set of coefficients, we need to add a loop to update each one:\n\ndef update_coeffs(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nThat’s it – we’re now ready to train our model!\n\ncoeffs = train_model(lr=1.4)\n\n\ncoeffs = train_model(lr=20)\n\nIt’s looking good – our loss is lower than before. Let’s see if that translates to a better result on the validation set:\n\nacc(coeffs)\n\nIn this case our neural net isn’t showing better results than the linear model. That’s not surprising; this dataset is very small and very simple, and isn’t the kind of thing we’d expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#deep-learning",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#deep-learning",
    "title": "My Notebook 2",
    "section": "Deep learning",
    "text": "Deep learning\nThe neural net in the previous section only uses one hidden layer, so it doesn’t count as “deep” learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.\nFirst, we’ll need to create additional coefficients for each layer:\n\ndef init_coeffs():\n    hiddens = [10, 10]  # &lt;-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\nYou’ll notice here that there’s a lot of messy constants to get the random numbers in just the right ranges. When you train the model in a moment, you’ll see that the tiniest changes to these initialisations can cause our model to fail to train at all! This is a key reason that deep learning failed to make much progress in the early days – it’s very finicky to get a good starting point for our coefficients. Nowadays, we have ways to deal with that, which we’ll learn about in other notebooks.\nOur deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately:\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.sigmoid(res)\n\nWe also need a minor update to update_coeffs since we’ve got layers and consts separated now:\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nLet’s train our model…\n\ncoeffs = train_model(lr=4)\n\n…and check its accuracy:\n\nacc(coeffs)"
  },
  {
    "objectID": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#final-thoughts",
    "href": "posts/from_scratch_model/05-linear-model-and-neural-net-from-scratch.html#final-thoughts",
    "title": "My Notebook 2",
    "section": "Final thoughts",
    "text": "Final thoughts\nIt’s actually pretty cool that we’ve managed to create a real deep learning model from scratch and trained it to get over 80% accuracy on this task, all in the course of a single notebook!\nThe “real” deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you’ll recognise the basic steps are the same.\nThe biggest differences in practical models to what we have above are:\n\nHow initialisation and normalisation is done to ensure the model trains correctly every time\nRegularization (to avoid over-fitting)\nModifying the neural net itself to take advantage of knowledge of the problem domain\nDoing gradient descent steps on smaller batches, rather than the whole dataset.\n\nI’ll be adding notebooks about all these later, and will add links here once they’re ready.\nIf you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you’re looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won’t get counted!) And if you have any questions or comments, please pop them below – I read every comment I receive!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "seanlewis08.github.io",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nMy Notebook 2\n\n\n\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\n\n\n17 min\n\n\n\n\n\n\n\nIntroduction and set up\n\n\n\n\n\n\n\n\n\n\n\n7 min\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n13 min\n\n\n\n\n\n\n\nMy Notebook\n\n\n\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\n1 min\n\n\n\n\n\n\n\nThis is a dummy blog posts\n\n\n\n\n\n\n123\n\n\nSecond Tag\n\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\n\n\nJun 1, 2022\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/first_blog_post/main.html",
    "href": "posts/first_blog_post/main.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;%\n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top',\n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;%\n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;%\n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top',\n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/first_blog_post/main.html#merriweather",
    "href": "posts/first_blog_post/main.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;%\n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top',\n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;%\n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;%\n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top',\n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/first_blog_post/main.html#columns",
    "href": "posts/first_blog_post/main.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/first_blog_post/main.html#margin-captions",
    "href": "posts/first_blog_post/main.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html",
    "title": "Introduction and set up",
    "section": "",
    "text": "If you’ve finished going through my Linear model and neural net from scratch notebook, then now is a good time to look at how to do the same thing using a library, instead of doing it from scratch. We’ll use fastai and PyTorch. The benefits of using these libraries is:- Best practices are handled for you automatically – fast.ai has done thousands of hours of experiments to figure out what the best settings are for you- Less time getting set up, which means more time to try out your new ideas- Each idea you try will be less work, because fastai and PyTorch will do the many of the menial bits for you- You can always drop down from fastai to PyTorch if you need to customise any part (or drop down from the fastai Application API to the fastai mid or low tier APIs), or even drop down from PyTorch to plain python for deep customisation.Let’s see how that looks in practice. We’ll start by doing the same library setup as in the “from scratch” notebook:\n::: {#cell-3 .cell _kg_hide-output=‘true’ execution=‘{“iopub.status.busy”:“2022-05-16T21:26:52.397584Z”,“iopub.execute_input”:“2022-05-16T21:26:52.398131Z”,“iopub.status.idle”:“2022-05-16T21:27:20.193935Z”,“shell.execute_reply.started”:“2022-05-16T21:26:52.398019Z”,“shell.execute_reply”:“2022-05-16T21:27:20.193065Z”}’ trusted=‘true’}\n:::\nWe’ll import the fastai tabular library, set a random seed so the notebook is reproducible, and pick a reasonable number of significant figures to display in our tables:\nfrom fastai.tabular.all import *pd.options.display.float_format = '{:.2f}'.formatset_seed(42)"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#prep-the-data",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#prep-the-data",
    "title": "Introduction and set up",
    "section": "Prep the data",
    "text": "Prep the data\nWe’ll read the CSV file just like we did before:\n\ndf = pd.read_csv(path/'train.csv')\n\nWhen you do everything from scratch, every bit of feature engineering requires a whole lot of work, since you have to think about things like dummy variables, normalization, missing values, and so on. But with fastai that’s all done for you. So let’s go wild and create lots of new features! We’ll use a bunch of the most interesting ones from this fantastic Titanic feature engineering notebook (and be sure to click that link and upvote that notebook if you like it to thank the author for their hard work!)\n\ndef add_features(df):    df['LogFare'] = np.log1p(df['Fare'])    df['Deck'] = df.Cabin.str[0].map(dict(A=\"ABC\", B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\"))    df['Family'] = df.SibSp+df.Parch    df['Alone'] = df.Family==1    df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')    df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]    df['Title'] = df.Title.map(dict(Mr=\"Mr\",Miss=\"Miss\",Mrs=\"Mrs\",Master=\"Master\")).value_counts(dropna=False)add_features(df)\n\nAs we discussed in the last notebook, we can use RandomSplitter to separate out the training and validation sets:\n\nsplits = RandomSplitter(seed=42)(df)\n\nNow the entire process of getting the data ready for training requires just this one cell!:\n\ndls = TabularPandas(    df, splits=splits,    procs = [Categorify, FillMissing, Normalize],    cat_names=[\"Sex\",\"Pclass\",\"Embarked\",\"Deck\", \"Title\"],    cont_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Alone', 'TicketFreq', 'Family'],    y_names=\"Survived\", y_block = CategoryBlock(),).dataloaders(path=\".\")\n\nHere’s what each of the parameters means:- Use splits for indices of training and validation sets: splits=splits, - Turn strings into categories, fill missing values in numeric columns with the median, normalise all numeric columns: procs = [Categorify, FillMissing, Normalize], - These are the categorical independent variables: cat_names=[“Sex”,“Pclass”,“Embarked”,“Deck”, “Title”], - These are the continuous independent variables: cont_names=[‘Age’, ‘SibSp’, ‘Parch’, ‘LogFare’, ‘Alone’, ‘TicketFreq’, ‘Family’], - This is the dependent variable: y_names=“Survived”,- The dependent variable is categorical (so build a classification model, not a regression model): y_block = CategoryBlock(),"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#train-the-model",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#train-the-model",
    "title": "Introduction and set up",
    "section": "Train the model",
    "text": "Train the model\nThe data and model together make up a Learner. To create one, we say what the data is (dls), and the size of each hidden layer ([10,10]), along with any metrics we want to print along the way:\n\nlearn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n\nYou’ll notice we didn’t have to do any messing around to try to find a set of random coefficients that will train correctly – that’s all handled automatically.One handy feature that fastai can also tell us what learning rate to use:\n\nlearn.lr_find(suggest_funcs=(slide, valley))\n\nThe two colored points are both reasonable choices for a learning rate. I’ll pick somewhere between the two (0.03) and train for a few epochs:\n\nlearn.fit(16, lr=0.03)\n\nWe’ve got a similar accuracy to our previous “from scratch” model – which isn’t too surprising, since as we discussed, this dataset is too small and simple to really see much difference. A simple linear model already does a pretty good job. But that’s OK – the goal here is to show you how to get started with deep learning and understand how it really works, and the best way to do that is on small and easy to understand datasets."
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#submit-to-kaggle",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#submit-to-kaggle",
    "title": "Introduction and set up",
    "section": "Submit to Kaggle",
    "text": "Submit to Kaggle\nOne important feature of fastai is that all the information needed to apply the data transformations and the model to a new dataset are stored in the learner. You can call export to save it to a file to use it later in production, or you can use the trained model right away to get predictions on a test set.To submit to Kaggle, we’ll need to read in the test set, and do the same feature engineering we did for the training set:\n\ntst_df = pd.read_csv(path/'test.csv')tst_df['Fare'] = tst_df.Fare.fillna(0)add_features(tst_df)\n\nBut we don’t need to manually specify any of the processing steps necessary to get the data ready for modeling, since that’s all saved in the learner. To specify we want to apply the same steps to a new dataset, use the test_dl() method:\n\ntst_dl = learn.dls.test_dl(tst_df)\n\nNow we can use get_preds to get the predictions for the test set:\n\npreds,_ = learn.get_preds(dl=tst_dl)\n\nFinally, let’s create a submission CSV just like we did in the previous notebook…\n\ntst_df['Survived'] = (preds[:,1]&gt;0.5).int()sub_df = tst_df[['PassengerId','Survived']]sub_df.to_csv('sub.csv', index=False)\n\n…and check that it looks reasonable:\n\n!head sub.csv"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#ensembling",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#ensembling",
    "title": "Introduction and set up",
    "section": "Ensembling",
    "text": "Ensembling\nSince it’s so easy to create a model now, it’s easier to play with more advanced modeling approaches. For instance, we can create five separate models, each trained from different random starting points, and average them. This is the simplest approach of ensembling models, which combines multiple models to generate predictions that are better than any of the single models in the ensemble.To create our ensemble, first we copy the three steps we used above to create and train a model, and apply it to the test set:\n\ndef ensemble():    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])    with learn.no_bar(),learn.no_logging(): learn.fit(16, lr=0.03)    return learn.get_preds(dl=tst_dl)[0]\n\nNow we run this five times, and collect the results into a list:\n\nlearns = [ensemble() for _ in range(5)]\n\nWe stack this predictions together and take their average predictions:\n\nens_preds = torch.stack(learns).mean(0)\n\nFinally, use the same code as before to generate a submission file, which we can submit to Kaggle after the notebook is saved and run:\n\ntst_df['Survived'] = (ens_preds[:,1]&gt;0.5).int()sub_df = tst_df[['PassengerId','Survived']]sub_df.to_csv('ens_sub.csv', index=False)\n\nAt the time of writing, this submission is well within the top 25% of entries to the competition.(A lot of submissions to this competition use additional external data, but we have restricted ourselves to just using the data provided. We’d probably do a lot better if we used external data too. Feel free to give that a try, and see how you go. Note that you’ll never be able to get to the top of the leaderboard, since a lot of folks in this competition have cheated, by downloading the answers from the internet and uploading them as their submission. In a real competition that’s not possible, because the answers aren’t public, but there’s nothing stopping people from cheating in a tutorial/practice competition like this one. So if you’re ready for a real challenge, take a look at the competitions page and start working on a real competition!)"
  },
  {
    "objectID": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#final-thoughts",
    "href": "posts/from_scratch_model/06-why-you-should-use-a-framework.html#final-thoughts",
    "title": "Introduction and set up",
    "section": "Final thoughts",
    "text": "Final thoughts\nAs you can see, using fastai and PyTorch made things much easier than doing it from scratch, but it also hid away a lot of the details. So if you only ever use a framework, you’re not going to as fully understand what’s going on under the hood. That understanding can be really helpful when it comes to debugging and improving your models. But do use fastai when you’re creating models on Kaggle or in “real life”, because otherwise you’re not taking advantage of all the research that’s gone into optimising the models for you, and you’ll end up spending more time debugging and implementing menial boiler-plate than actually solving the real problem!If you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you’re looking at my original notebook here when you do that, and are not on your own copy of it, otherwise your upvote won’t get counted!) And if you have any questions or comments, please pop them below – I read every comment I receive!"
  },
  {
    "objectID": "posts/Jupyter_example/main.html",
    "href": "posts/Jupyter_example/main.html",
    "title": "My Notebook",
    "section": "",
    "text": "This is a Test\n\nx = 4 + 5\nprint(x)\n9\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\nPolar axis plot"
  }
]